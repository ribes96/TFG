\subsection{General Idea}
  La idea general un poco desarroyada

  \begin{note}
    Hemos visto que podemos sacar una aproximación aleatoria de la función
    implícita de un kernel. Esto tiene básicamente 2 ventajas:
    \begin{itemize}
      \item Podemos transformar los datos directamente
      \item Podemos producir pequeñas variaciones de un mismo dataset, todas
      ellas válidas.
    \end{itemize}

    \begin{description}
      \item[Black Bag] Datos \textrightarrow\ RFF \textrightarrow\ Bootstrap \textrightarrow\ Modelos
      \item[Grey Bag]
      \item[Black Ensemble]
      \item[Grey Ensemble]
    \end{description}

    La figura \ref{figure:box_models} muestra los tipos de caja
  \end{note}

  \boxPics

  Las funciones kernel son funciones que se pueden expresar de la forma:

  \begin{equation}
     \kappa(\vx, \vy) = \phi(\vx)^T\phi(\vy)
  \end{equation}
  Es decir, como producto escalar de una función de sus parámetros. Un kernel muy
  popular es el RBF (\eng{Radial Basis Function}) gausiano, que es este:

  \begin{equation}
   \kappa({\vx, \vy}; \sigma) = e^{-\frac{\norm{\vx - \vy}}{2\sigma^2}}
  \end{equation}

  La función implícita $\phi$ ($\mathcal{L} \mapsto \mathcal{H}$) de este kernel
  tiene una dimensionalidad infinita, y se sabe que para cualquier conjunto de
  datos se puede encontrar un kernel RBF $\kappa$ tal que su función implícita
  $\phi$ es capaz de separarlos mediante un híper-plano.

  A pesar de que la función $\phi$ tiene dimensionalidad infinita (
  $\mathcal{H} \equiv \reals^\infty$
  ) es posible extraer una aproximación aleatoria de la misma con una precisión arbitraria,
  mediante el uso de \eng{Random Fourier Features} \cite{rff} (RFF). Otra técnica
  que también se puede utilizar es el método Nystroem. Con estos
  métodos se puede extraer $\psi(\vx) \approx \phi(\vx)$ y usarlos para lo que
  haga falta.

  La extracción de estas aproximaciones se ha usado con anterioridad junto con
  métodos de redes neuronales, y ha mostrado muy buenos resultados. En este
  nosotros tratamos de usarlas con otros modelos. En particular, hemos
  estudiado los modelos de Decision Tree, Logit y LinearSVC, en combinación con
  varios tipos de ensemble.

  El uso de ensembles está muy extendido junto con los Decision Tree. Esto se
  debe a que éste es un modelo muy inestable, y una pequeña alteración en los
  datos puede producir resultados muy distintos. Estas condiciones son idoneas
  para hacer un comité de Decision Trees, entrenarlos con datos ligeramente
  distintos y elegir la solución qué más árboles hayan predicho.

  Pero este procedimiento no tiene ningún sentido hacerlo con modelos que no son
  inestables. Si los modelos no son inestables, la mayoría de los estimadores
  responderán la misma solución, y no servirá para nada haber entrenado tantos
  modelos distintos. Es como si en un comité de expertos todos ellos opinaran
  igual: para eso no necesitamos todo un comité, con un solo experto nos habría
  bastado.

  La técnica de \eng{bagging} utiliza el \eng{bootstrap} para generar datasets
  distintos para cada uno de los estimadores. Consiste en hacer un remuestreo de
  los datos con repetición para cada uno de los estimadores. Esta diferenciación
  que se produce es suficiente para los Decision Tree, pero es demasiado leve con
  los métodos más estables, como Logit y LinearSVC.

  Pero los RFF y Nystroem abren una nueva puerta. Puesto que son aproximaciones
  aleatorias de un conjunto infinito, podemos sacar tantos mapeos distintos
  como queramos de los datos originales, y por lo tanto podemos diferenciar
  todavía más los datasets generados para cada uno de los estimadores.

  Además de todo esto, hay una ventaja adicional: entrenar una \eng{Support
  Vector Machine} (SVM) con kernel lineal es más barato que entrenar una no lineal, por
  ejemplo una que use RBF. Si usamos una SVM lineal, pero en vez de entrenarla
  con los datos originales la entrenamos con los datos $\psi(\vx)$, tenemos un
  coste similar al de entrenar una SVM lineal pero con una precisión equiparable
  a una RBF-SVM. Esto ya se ha hecho antes.

  Existen varias formas de combinar las RFF con los métodos ensembles. Básicamente,
  hay dos parámetros que podemos elegir: qué tipo de ensemble usar y en qué
  momento usar las RFF.

  Cuando se combina un ensemble con los RFF, básicamente hay dos momentos en
  los que se puede usar el mapeo. Un momento es nada más empezar, antes de
  que el ensemble haya visto los datos, y el ensemble trabaja normalmente, solo
  que en vez de recibir los datos originales recibe un mapeo de los mismos.
  Este método se abstrae completamente de lo que hace el ensemble, y lo trata
  como una caja negra. \eng{Black Box}.

  El otro método consiste en usar el RFF, no nada más empezar y el mismo para
  todos los estimadores, sino justo antes del estimador: se hace un mapeo nuevo
  para cada uno de los estimadores. Este método ya se mete dentro de lo que es
  un ensemble, y por tanto diremos que es de caja blanca (\eng{White Box}).

  Pero se sabe que se obtienen mejores resultados cuando hay bastante diversidad
  entre los estimadores del ensemble. Se nos presentan ahora dos formas de crear
  diversidad en el ensemble. Una de ellas es la forma clásica, mediante el
  bootstrap, que ha mostrado muy buenos resultados con el \eng{Decision Tree}.
  Pero ahora podemos usar también la aleatoriedad de los RFF para generar
  esa diversidad. Entonces tenemos dos opciones: usar los RFF ellos solos o usarlos
  junto con el Bootstrap. A usarlos junto con el bootstrap le llamaremos un
  \eng{Bagging}, mientras que si no usamos bootstrap le llamaemos un \eng{Ensemble}.

  Tenemos entonces varias combinaciones entre manos:

  \paragraph{Black Bag}
  Black Box model con Bagging. Primero se hace un mapeo de los datos y después se
  hace un bootstrap con ellos para cada uno de los estimadores. Si los estimadores
  son \eng{Decision Tree} es los mismo que un \eng{Random Forest}, pero no con los
  datos originales, sino con el mapeo.

  \paragraph{White Bag}
  White Box model con Bagging. Primero se hace un bootstrap de los datos, para
  cada uno de los modelos, y después para cada uno de ellos se hace un mapeo de
  los datos.

  \paragraph{White Ensemble}
  White Box model sin baging. Se hace un mapeo para cada uno de los estimadores,
  todos ellos usando todos los datos originales.


  El \eng{Black Ensemble} no tiene ningún sentido hacerlo, porque en ese caso
  todos los estimadores recibirían exactamente los mismos datos, y por lo tanto
  todos producirían exactamente los mismos resutados, a no ser que tuvieran algún
  tipo de aleatoriedad, como los Decision Tree. A pesar de que tengamos ese caso
  particular con los DT, no lo vamos a tratar.

  Y luego, por supuesto, haremos pruebas con un modelo simple usando los RFF, sin
  usar ningún tipo de ensemble.

\begin{note}
  \subsubsection{State of the art con las RFF}

  Se ha trabajado muy poco con ellas. Básicamente, solo se le ha dado
  dos usos:
  \begin{description}
    \item[Stacked kernel network] \cite{stacked_kernel_network} Usarlas junto
    con una red neuronal para tener más niveles de aprendizaje no lineal.
    \item[RFF with SVM] \cite{svm_rff} Usar una SVM sin kernel con los datos
    mapeados usando RFF
  \end{description}
\end{note}
