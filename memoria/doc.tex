\documentclass{article}
\usepackage[utf8]{inputenc}
% \usepackage[spanish]{babel}
\usepackage[english]{babel}

\usepackage{biblatex}
\addbibresource{sample.bib}
\usepackage{csquotes}
\usepackage{authblk}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{eurosym}
\usepackage{ upgreek }
\usepackage{bm}
\usepackage{ dsfont }
\usepackage{commath}
\usepackage{amsmath,amsfonts,amssymb,amsthm}

% \restylefloat{table}

\newcommand{\reals}{\mathds{R}}
\newcommand{\vx}{\bm{x}}
\newcommand{\vy}{\bm{y}}
\newcommand{\eng}[1]{\textit{#1}}
\newcommand{\transp}{\intercal}
\newcommand{\kernel}{\kappa}

% TODO no sé si algunos nombres se traducen
\title{Using Random Fourier Features with Random Forest}
\author{Albert Ribes}
% TODO poner la fecha adecuada
\date{Fecha de defensa}
\affil{Director: Lluís A. Belanche Muñoz}
\affil{Computer Science}
\affil{Grau en Enginyeria Informàtica}
\affil{Computació}
\affil{FACULTAT D’INFORMÀTICA DE BARCELONA (FIB)}
\affil{UNIVERSITAT POLITÈCNICA DE CATALUNYA (UPC) -- BarcelonaTech}
% Se deben incluir estos campos
% a) Títol
% b) Autor
% c) Data de defensa
% d) Director i Departament del Director
% e) Titulació
% f) Especialitat
% g) Centre: FACULTAT D’INFORMÀTICA DE BARCELONA (FIB)
% h) Universitat: UNIVERSITAT POLITÈCNICA DE CATALUNYA (UPC) – BarcelonaTech

\begin{document}
\maketitle
\tableofcontents
% \newpage

%%%%%%%%%%%%%%%%%%%%%%%%
%% Content starts here
%%%%%%%%%%%%%%%%%%%%%%%%


\section{Ideas generales}
El guión que me propuso LLuís es:
\begin{enumerate}
 \item Problema que ataco
 \item Por qué es importante
 \item Qué propongo en mi TFG
 \item Estado del arte en el problema que ataco
 \item Nociones generales del tema
       \begin{itemize}
        \item Machine Learning
        \item Árboles
        \item Logit
        \item RFF
        \item Nystroem
        \item Bootstrap
        \item Boosting
       \end{itemize}
 \item El trabajo propiamente dicho (explicar lo que voy a hacer)
 \item Experimentos
 \item Conclusiones y Trabajo futuro
 \item Referencias
 \item Apéndices
\end{enumerate}

\section{Introduction}
\subsection{Problem to solve}
Todavía no se consigue suficiente precisión con el Machine Learning
\subsection{Why it is important to solve this problem}
Con precisión más alta se podría aplicar el machine learning en otros campos
\subsection{Project proposal}
Incrementar el accuracy que se puede conseguir con algunos problemas mezclando
la técnica del bagging (y quizá del boosting) con el tema los RFF

Actualmente el bagging solo se usa con Decision Tree porque es muy inestable.
Con lo que propongo aquí, podría ser factible usarlo con otros algoritmos más
estables

\section{Background}
Más o menos, cada uno debería ocupar entre media y una página
\subsection{Machine Learning}
\subsection{Classification and Regression}
\subsection{Review de los principales modelos que existen}
\subsubsection{Decision Tree}
\subsubsection{Logistic Regression}
\subsubsection{SVM}
\subsection{Las técnicas ensembling}
\subsubsection{Bagging}
\begin{itemize}
 \item Inventado por Leo Breiman
 \item Pretende reducir el sesgo
\end{itemize}
\subsubsection{Boosting}
\begin{itemize}
 \item Adaboost (adaptive boosting)
 \item El siguiente estimador es más probable que contenga los elementos no
       no se han predicho bien en el anterior
 \item Se trata de ir modificando los pesos que tiene cada una de las instancias
 \item El entrenamiento de los modelos es secuencial, a diferencia del bagging
 \item Enterarme de quien lo inventó, y para qué ámbitos es útil
\end{itemize}
\subsection{El bootstrap}
\begin{itemize}
 \item En bagging es bueno que los estimadores estén poco relacionados
       entre ellos
 \item Idealmente, usaríamos un dataset distinto para cada uno de los
       estimadores, pero eso no siempre es posible
 \item Una alternativa es usar un resampling con repetición sobre cada
       uno de los estimadores para tener datasets un poco distintos entre ellos.
 \item Enterarme de la cantidad de elementos distintos que se espera que queden
       en el subconjunto, y quizá hablar de la cantidad de aleatoriedad
\end{itemize}
\subsection{Las funciones kernel}
Las SVM encuentran un híper-plano que separa las instancias de un problema
determinado en dos subconjuntos del espacio, y en el que cada subconjunto se
identifica con las clases que se quieren discriminar. Este híper-plano
busca maximizar la distancia mínima entre él mismo y las instancias (los vectores)
de cada una de las clases. Para hacerlo, convierte el problema en uno de
optimización.

Tenemos un conjunto de datos $D = \{\bm{\chi}, \bm{y}\}$, donde $\bm{\chi} = \{\bm{x}_1, \ldots \bm{x}_n\}$, $\bm{x}_i \in \reals^d$, $\bm{y} = \{-1, +1\}^n$

Se requiere encontrar $\bm{\alpha} \in \reals^n$ que maximice:

\begin{equation}
 \sum_i\alpha_i - \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\bm{x}_i^T\bm{x}_j
\end{equation}

% \begin{equation}
%   \sum_t\alpha^t - \frac{1}{2}\sum_t\sum_s\alpha^t\alpha^sr^tr^s\bm{\upphi}(\bm{x}^t)^T\bm{\upphi}(\bm{x}^s)
% \end{equation}
% sujeto a
%
% \begin{equation}
%   \sum_t\alpha^tr^t = 0 \textrm{ y } 0 \leq \alpha^t \leq C, \forall t
% \end{equation}

Pero este procedimiento solamente es efectivo si se da el caso que todas las
instancias de $\bm{\chi}$ son linealmente separables por un híper-plano en cada
una de las dos clases.

Este no siempre es el caso, y por eso se suele realizar una transformación de los
datos, que los lleven de un subespacio a otro, que normalmente tiene más dimensiones
que el original y que se espera que sí permita separar linealmente los datos.

Entonces, si se define una función de transformación de espacio
$z(\bm{x}) : \reals^d \rightarrow \reals^D$, la función a optimizar sería:

\begin{equation}
 \sum_i\alpha_i - \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_jz(\bm{x}_i)^Tz(\bm{x}_j)
\end{equation}

Estos cálculos únicamente trabajan con el producto escalar de los vectores, nunca
con ellos directamente. Es por eso que si existiera una función:

\begin{equation}
 \kappa(\vx, \vy) = z(\vx)^Tz(\vy)
\end{equation}

Se podría optimizar la función

\begin{equation}
 \sum_i\alpha_i - \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\kappa(\bm{x}_i, \bm{x}_j)
\end{equation}

sin tener jamás que calcular el producto escalar de los vectores. De hecho, la
dimensionalidad del nuevo espacio vectorial podría ser infinita sin ningún problema.
Lo único necesario sería que en ese nuevo espacio, que no tenemos por qué conocer,
los vectores fueran linealmente separables.

Pues estas funcions $\kappa$ existen, y se suelen llamar funciones kernel. Se usan
especialmente con las SVM, pero se podrían usar en cualquier otro campo.

Algunas de las que existen son el kernel lineal (
$\kappa(\vx, \vy) = \vx^T\vy + c$
), el polinómico (
$\kappa(\vx, \vy) = (\vx^T\vy + c)^d$
), el gausiano o RBF (
$\kappa(\vx, \vy) = exp(-\frac{\norm{\vx - \vy}^2}{2\sigma^2})$
), etc.

\subsubsection{El kernel RBF}

El kernel RBF es una familia de funciones kernel. El nombre viene de
\textit{Radial Basis Function}. Esta familia de funciones tiene un parámetro
$\sigma$, y son estas:

\begin{equation}
 \kappa({\vx, \vy}; \sigma) = e^{-\frac{\norm{\vx - \vy}}{2\sigma^2}}
\end{equation}

A veces también lo expresan con una gamma ($\gamma$), con la equivalencia
$\gamma = \frac{1}{2\sigma^2}$:

\begin{equation}
 \kappa({\vx, \vy}; \gamma) = e^{-\gamma\norm{\vx - \vy}}
\end{equation}

Tiene una cierta noción de similaridad entre los dos vectores: cuanto más
distintos son (cuanto mayor es la norma de su diferencia) más se aproxima a 0,
y si son iguales es 1.

% Tiene una cierta noción de similaridad entre los dos vectores: cuanto más
% parecidos son (cuanto menor es la norma de su diferencia) más se aproxima a 1, y
% cuanto más distintos, más cerca de 0.

Se sabe que el feature space de este kernel tiene dimensionalidad infinita, y
es de los kernel más utilizados.

(Me gustaría enterarme si siempre siempre siempre es dimensionalidad infinita,
con cualquier valor de gamma).

Un valor de $\sigma$ muy pequeño (muy cercano a 0) produce más sobre-ajuste,
mientras que un valor más grande lo disminuye.


\subsection{Las Random Fourier Features}

\begin{equation}
  \kernel(\lambda) \approx \langle\phi(\vx), \phi(\vy)\rangle
\end{equation}

\begin{equation}
  \omega_i \sim \kappa(\omega)
\end{equation}

\begin{equation}
  \phi(x) = \frac{1}{\sqrt{D}}\left[ e^{-i\omega^\transp_1x}, \ldots, e^{-i\omega^\transp_Dx} \right]^\transp
\end{equation}
Es una ténica que permite aproximar el feature space de un kernel. Sea $\kappa$
un kernel, tal que

\begin{equation}
 \kappa(\vx, \vy) = z(\vx)^Tz(\vy)
\end{equation}

(Creo que no permite aproximar todos los kernel, solo los que cumplen una condición)

Donde
$z(\vx) : \reals^d \rightarrow \reals^D$.
En el caso particular del
kernel RBF,
$z(\vx) : \reals^d \rightarrow \reals^\infty$

Las Random Fourier Features permiten generar una función $f(\vx)$ que aproxima $z(\vx)$
con una dimensionalidad arbitraria, de manera que
$f(\vx)f(\vy) \approx \kappa(\vx, \vy)$

Como el subespacio de $z(\vx)$ es de dimensionalidad infinita para algunos kernels
como el RBF, $f(\vx)$ coje un subconjunto aleatorio de todas esas dimensiones,
según la cantidad que se haya especificado. Esto permite generar varias imágenes
aleatorias de distintas aproximaciones $f(\vx)$ para un mismo vector $\vx$, y esto
mismo es lo que se explota en este trabajo para generar aleatoriedad en los datos

\subsection{Nystroem}

Sobretodo en el ámbito de las SVM se utiliza el concepto de \textit{Gramm matrix}
de un kernel entrenar un modelo. Sea
$\bm{\chi} = \{\vx_1, \ldots \vx_n\}$
un conjunto de datos y
$\kappa(\vx, \vy)$
un función kernel. La matriz de Gram $G$
es de tamaño $n \times n$, y
$G_{i,j} = \kappa(\vx_i, \vx_j)$

El cálculo de esta matriz es muy costoso en tiempo y en espacio, y por lo tanto
no es factible para la mayoría de problemas de Machine Learning, que requieren
grandes cantidades de datos.

El método Nystroem consiste en aproximar esta matriz de Gram con un subconjunto
aleatorio de los datos que sea adecuado sin afectar negativamente la precisión
de la solución
\subsection{PCA}
\subsection{Cross-validation}

\section{Workflow of the project}
\subsection{La idea general un poco desarroyada}

Las funciones kernel son funciones que se pueden expresar de la forma:

\begin{equation}
   \kappa(\vx, \vy) = \phi(\vx)^T\phi(\vy)
\end{equation}
Es decir, como producto escalar de una función de sus parámetros. Un kernel muy
popular es el RBF (\eng{Radial Basis Function}) gausiano, que es este:

\begin{equation}
 \kappa({\vx, \vy}; \sigma) = e^{-\frac{\norm{\vx - \vy}}{2\sigma^2}}
\end{equation}

La función implícita $\phi$ ($\mathcal{L} \mapsto \mathcal{H}$) de este kernel
tiene una dimensionalidad infinita, y se sabe que para cualquier conjunto de
datos se puede encontrar un kernel RBF $\kappa$ tal que su función implícita
$\phi$ es capaz de separarlos mediante un híper-plano.

A pesar de que la función $\phi$ tiene dimensionalidad infinita
($\mathcal{H} \equiv \reals^\infty$), es posible

El kernel RBF se utiliza mucho en Machine Learning, especialmente en SVM, donde
lo único importante es el producto escalar entre los vectores de datos. Este
kernel realiza una transformación implícita de los datos a un espacio de
dimensionalidad infinita en el que, en el caso de las SVM, se espera que los
datos sean separables con un híper-plano.

Este nuevo espacio de dimensionalidad infinita

\subsection{State of the art con las RFF}



\section{Experimental results}
\section{Conclussion}
De momento, parece que algunos problemas sí que se benefician de esto, mientas
que otros no lo hacen
\section{Future work}
\begin{itemize}
 \item El trabajo se ha centrado en problemas de clasificación, pero no hay
       ningún motivo para que no se pueda aplicar el regresión. Se ha omitodo por
       simplificar
 \item Aquella teoría de que quizá se puede regular la cantidad de aleatoriedad
       que añade el bootstrap, y quizá inventar un bootstrap con un parámetro para
       regular la cantidad de aleatoriedad
 \item Pensar en aquella teoría de que quizá se puede inventar un procedimiento
       para, dato un problema determinado con sus datos, sacar un número que sea
       representativo de la cantidad promedio de ruido que tiene. Puesto que quizá
       es útil para este proyecto conocer la cantidad de aleatoriedad que tienen
       los datos, para que se pueda regular
\end{itemize}
\section{Sustainability Report}
\input{sustainability_report}

% Aquí empieza el índice que tenía en la fita de seguiment
% \section{Context}
%     \subsection{General Framework}
%     \subsection{Into the specifics}
%     \subsection{State of the Art}
%     \subsection{Problem to solve}
%
% \section{Planning}
%     \subsection{Original Planning}
%     \subsection{Problems encountered with original planning}
%     \subsection{Proposed new planning}
%
% \section{Methodology}
%     \subsection{Original Proposed Methodology}
%     \subsection{Problems encountered with original methodology}
%     \subsection{New methodology}
%
% \section{Alternatives Analysis}
%     \subsection{Language for development}
%     \subsection{Running environment}
%     \subsection{Machine Learning Algorithms}
%
% \section{Knowledge Integration}
%
% \section{Implication and Decision Making}
%     \subsection{Meetings with director}
%     \subsection{Goals achievement}
%     \subsection{Rigour in scientific procedures}
%
% \section{Laws and regulations}
%     \subsection{My responsibility}
%     \subsection{Others responsibility}
%     Esto es algo que hizo \cite{dirac} y también \cite{einstein}








\printbibliography
\end{document}
