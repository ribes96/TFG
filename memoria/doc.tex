\documentclass{article}
\usepackage[utf8]{inputenc}
% \usepackage[spanish]{babel}
\usepackage[english]{babel}

\usepackage{biblatex}
\addbibresource{ribes19bib.bib}
\usepackage{csquotes}
\usepackage{authblk}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{eurosym}
\usepackage{ upgreek }
\usepackage{bm}
\usepackage{ dsfont }
\usepackage{commath}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{ifthen}
\usepackage{comment}


% \restylefloat{table}

\usepackage{mystyle}

% \newcommand{\reals}{\mathds{R}}
% \newcommand{\vx}{\bm{x}}
% \newcommand{\vy}{\bm{y}}
% \newcommand{\eng}[1]{\textit{#1}}
% \newcommand{\transp}{\intercal}
% \newcommand{\kernel}{\kappa}

% TODO no sé si algunos nombres se traducen
\title{Using Random Fourier Features with Random Forest}
\author{Albert Ribes}
% TODO poner la fecha adecuada
\date{Fecha de defensa}
\affil{Director: Lluís A. Belanche Muñoz}
\affil{Computer Science}
\affil{Grau en Enginyeria Informàtica}
\affil{Computació}
\affil{FACULTAT D’INFORMÀTICA DE BARCELONA (FIB)}
\affil{UNIVERSITAT POLITÈCNICA DE CATALUNYA (UPC) -- BarcelonaTech}
% Se deben incluir estos campos
% a) Títol
% b) Autor
% c) Data de defensa
% d) Director i Departament del Director
% e) Titulació
% f) Especialitat
% g) Centre: FACULTAT D’INFORMÀTICA DE BARCELONA (FIB)
% h) Universitat: UNIVERSITAT POLITÈCNICA DE CATALUNYA (UPC) – BarcelonaTech

% Cuando sea texto temporal, esto evitará que se pinte
\newboolean{delivery}
\setboolean{delivery}{false}


\begin{document}
\maketitle
\tableofcontents
% \newpage

%%%%%%%%%%%%%%%%%%%%%%%%
%% Content starts here
%%%%%%%%%%%%%%%%%%%%%%%%


\section{Ideas generales}

El guión que me propuso LLuís es:
\begin{enumerate}
 \item Problema que ataco
 \item Por qué es importante
 \item Qué propongo en mi TFG
 \item Estado del arte en el problema que ataco
 \item Nociones generales del tema
       \begin{itemize}
        \item Machine Learning
        \item Árboles
        \item Logit
        \item RFF
        \item Nystroem
        \item Bootstrap
        \item Boosting
       \end{itemize}
 \item El trabajo propiamente dicho (explicar lo que voy a hacer)
 \item Experimentos
 \item Conclusiones y Trabajo futuro
 \item Referencias
 \item Apéndices
\end{enumerate}

\section{Introduction}
\subsection{Problem to solve}
Todavía no se consigue suficiente precisión con el Machine Learning
\subsection{Why it is important to solve this problem}
Con precisión más alta se podría aplicar el machine learning en otros campos
\subsection{Project proposal}
Incrementar el accuracy que se puede conseguir con algunos problemas mezclando
la técnica del bagging (y quizá del boosting) con el tema los RFF

Actualmente el bagging solo se usa con Decision Tree porque es muy inestable.
Con lo que propongo aquí, podría ser factible usarlo con otros algoritmos más
estables

\section{Background}
Más o menos, cada uno debería ocupar entre media y una página
\subsection{Machine Learning}
\subsection{Classification and Regression}
\subsection{Review de los principales modelos que existen}
\subsubsection{Decision Tree}
\subsubsection{Logistic Regression}
\subsubsection{SVM}
\subsection{Las técnicas ensembling}
\subsubsection{Bagging}
\begin{itemize}
 \item Inventado por Leo Breiman
 \item Pretende reducir el sesgo
\end{itemize}
\subsubsection{Boosting}
\begin{itemize}
 \item Adaboost (adaptive boosting)
 \item El siguiente estimador es más probable que contenga los elementos no
       no se han predicho bien en el anterior
 \item Se trata de ir modificando los pesos que tiene cada una de las instancias
 \item El entrenamiento de los modelos es secuencial, a diferencia del bagging
 \item Enterarme de quien lo inventó, y para qué ámbitos es útil
\end{itemize}
\subsection{El bootstrap}
\begin{itemize}
 \item En bagging es bueno que los estimadores estén poco relacionados
       entre ellos
 \item Idealmente, usaríamos un dataset distinto para cada uno de los
       estimadores, pero eso no siempre es posible
 \item Una alternativa es usar un resampling con repetición sobre cada
       uno de los estimadores para tener datasets un poco distintos entre ellos.
 \item Enterarme de la cantidad de elementos distintos que se espera que queden
       en el subconjunto, y quizá hablar de la cantidad de aleatoriedad
\end{itemize}
\subsection{Las funciones kernel}
Las SVM encuentran un híper-plano que separa las instancias de un problema
determinado en dos subconjuntos del espacio, y en el que cada subconjunto se
identifica con las clases que se quieren discriminar. Este híper-plano
busca maximizar la distancia mínima entre él mismo y las instancias (los vectores)
de cada una de las clases. Para hacerlo, convierte el problema en uno de
optimización.

Tenemos un conjunto de datos $D = \{\bm{\chi}, \bm{y}\}$, donde $\bm{\chi} = \{\bm{x}_1, \ldots \bm{x}_n\}$, $\bm{x}_i \in \reals^d$, $\bm{y} = \{-1, +1\}^n$

Se requiere encontrar $\bm{\alpha} \in \reals^n$ que maximice:

\begin{equation}
 \sum_i\alpha_i - \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\bm{x}_i^T\bm{x}_j
\end{equation}

% \begin{equation}
%   \sum_t\alpha^t - \frac{1}{2}\sum_t\sum_s\alpha^t\alpha^sr^tr^s\bm{\upphi}(\bm{x}^t)^T\bm{\upphi}(\bm{x}^s)
% \end{equation}
% sujeto a
%
% \begin{equation}
%   \sum_t\alpha^tr^t = 0 \textrm{ y } 0 \leq \alpha^t \leq C, \forall t
% \end{equation}

Pero este procedimiento solamente es efectivo si se da el caso que todas las
instancias de $\bm{\chi}$ son linealmente separables por un híper-plano en cada
una de las dos clases.

Este no siempre es el caso, y por eso se suele realizar una transformación de los
datos, que los lleven de un subespacio a otro, que normalmente tiene más dimensiones
que el original y que se espera que sí permita separar linealmente los datos.

Entonces, si se define una función de transformación de espacio
$z(\bm{x}) : \reals^d \rightarrow \reals^D$, la función a optimizar sería:

\begin{equation}
 \sum_i\alpha_i - \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_jz(\bm{x}_i)^Tz(\bm{x}_j)
\end{equation}

Estos cálculos únicamente trabajan con el producto escalar de los vectores, nunca
con ellos directamente. Es por eso que si existiera una función:

\begin{equation}
 \kappa(\vx, \vy) = z(\vx)^Tz(\vy)
\end{equation}

Se podría optimizar la función

\begin{equation}
 \sum_i\alpha_i - \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\kappa(\bm{x}_i, \bm{x}_j)
\end{equation}

sin tener jamás que calcular el producto escalar de los vectores. De hecho, la
dimensionalidad del nuevo espacio vectorial podría ser infinita sin ningún problema.
Lo único necesario sería que en ese nuevo espacio, que no tenemos por qué conocer,
los vectores fueran linealmente separables.

Pues estas funcions $\kappa$ existen, y se suelen llamar funciones kernel. Se usan
especialmente con las SVM, pero se podrían usar en cualquier otro campo.

Algunas de las que existen son el kernel lineal (
$\kappa(\vx, \vy) = \vx^T\vy + c$
), el polinómico (
$\kappa(\vx, \vy) = (\vx^T\vy + c)^d$
), el gausiano o RBF (
$\kappa(\vx, \vy) = exp(-\frac{\norm{\vx - \vy}^2}{2\sigma^2})$
), etc.

\subsubsection{El kernel RBF}

El kernel RBF es una familia de funciones kernel. El nombre viene de
\textit{Radial Basis Function}. Esta familia de funciones tiene un parámetro
$\sigma$, y son estas:

\begin{equation}
 \kappa({\vx, \vy}; \sigma) = e^{-\frac{\norm{\vx - \vy}}{2\sigma^2}}
\end{equation}

A veces también lo expresan con una gamma ($\gamma$), con la equivalencia
$\gamma = \frac{1}{2\sigma^2}$:

\begin{equation}
 \kappa({\vx, \vy}; \gamma) = e^{-\gamma\norm{\vx - \vy}}
\end{equation}

Tiene una cierta noción de similaridad entre los dos vectores: cuanto más
distintos son (cuanto mayor es la norma de su diferencia) más se aproxima a 0,
y si son iguales es 1.

% Tiene una cierta noción de similaridad entre los dos vectores: cuanto más
% parecidos son (cuanto menor es la norma de su diferencia) más se aproxima a 1, y
% cuanto más distintos, más cerca de 0.

Se sabe que el feature space de este kernel tiene dimensionalidad infinita, y
es de los kernel más utilizados.

(Me gustaría enterarme si siempre siempre siempre es dimensionalidad infinita,
con cualquier valor de gamma).

Un valor de $\sigma$ muy pequeño (muy cercano a 0) produce más sobre-ajuste,
mientras que un valor más grande lo disminuye.


\subsection{Las Random Fourier Features}

\begin{equation}
  \kernel(\lambda) \approx \langle\phi(\vx), \phi(\vy)\rangle
\end{equation}

\begin{equation}
  \omega_i \sim \kappa(\omega)
\end{equation}

\begin{equation}
  \phi(x) = \frac{1}{\sqrt{D}}\left[ e^{-i\omega^\transp_1x}, \ldots, e^{-i\omega^\transp_Dx} \right]^\transp
\end{equation}
Es una ténica que permite aproximar el feature space de un kernel. Sea $\kappa$
un kernel, tal que

\begin{equation}
 \kappa(\vx, \vy) = z(\vx)^Tz(\vy)
\end{equation}

(Creo que no permite aproximar todos los kernel, solo los que cumplen una condición)

Donde
$z(\vx) : \reals^d \rightarrow \reals^D$.
En el caso particular del
kernel RBF,
$z(\vx) : \reals^d \rightarrow \reals^\infty$

Las Random Fourier Features permiten generar una función $f(\vx)$ que aproxima $z(\vx)$
con una dimensionalidad arbitraria, de manera que
$f(\vx)f(\vy) \approx \kappa(\vx, \vy)$

Como el subespacio de $z(\vx)$ es de dimensionalidad infinita para algunos kernels
como el RBF, $f(\vx)$ coje un subconjunto aleatorio de todas esas dimensiones,
según la cantidad que se haya especificado. Esto permite generar varias imágenes
aleatorias de distintas aproximaciones $f(\vx)$ para un mismo vector $\vx$, y esto
mismo es lo que se explota en este trabajo para generar aleatoriedad en los datos

\subsection{Nystroem}

Sobretodo en el ámbito de las SVM se utiliza el concepto de \textit{Gramm matrix}
de un kernel entrenar un modelo. Sea
$\bm{\chi} = \{\vx_1, \ldots \vx_n\}$
un conjunto de datos y
$\kappa(\vx, \vy)$
un función kernel. La matriz de Gram $G$
es de tamaño $n \times n$, y
$G_{i,j} = \kappa(\vx_i, \vx_j)$

El cálculo de esta matriz es muy costoso en tiempo y en espacio, y por lo tanto
no es factible para la mayoría de problemas de Machine Learning, que requieren
grandes cantidades de datos.

El método Nystroem consiste en aproximar esta matriz de Gram con un subconjunto
aleatorio de los datos que sea adecuado sin afectar negativamente la precisión
de la solución
\subsection{PCA}
\subsection{Cross-validation}

\section{Workflow of the project}
\subsection{La idea general un poco desarroyada}

Las funciones kernel son funciones que se pueden expresar de la forma:

\begin{equation}
   \kappa(\vx, \vy) = \phi(\vx)^T\phi(\vy)
\end{equation}
Es decir, como producto escalar de una función de sus parámetros. Un kernel muy
popular es el RBF (\eng{Radial Basis Function}) gausiano, que es este:

\begin{equation}
 \kappa({\vx, \vy}; \sigma) = e^{-\frac{\norm{\vx - \vy}}{2\sigma^2}}
\end{equation}

La función implícita $\phi$ ($\mathcal{L} \mapsto \mathcal{H}$) de este kernel
tiene una dimensionalidad infinita, y se sabe que para cualquier conjunto de
datos se puede encontrar un kernel RBF $\kappa$ tal que su función implícita
$\phi$ es capaz de separarlos mediante un híper-plano.

A pesar de que la función $\phi$ tiene dimensionalidad infinita (
$\mathcal{H} \equiv \reals^\infty$
) es posible extraer una aproximación aleatoria de la misma con una precisión arbitraria,
mediante el uso de \eng{Random Fourier Features} \cite{rff} (RFF). Otra técnica
que también se puede utilizar es el método Nystroem. Con estos
métodos se puede extraer $\psi(\vx) \approx \phi(\vx)$ y usarlos para lo que
haga falta.

La extracción de estas aproximaciones se ha usado con anterioridad junto con
métodos de redes neuronales, y ha mostrado muy buenos resultados. En este
nosotros tratamos de usarlas con otros modelos. En particular, hemos
estudiado los modelos de Decision Tree, Logit y LinearSVC, en combinación con
varios tipos de ensemble.

El uso de ensembles está muy extendido junto con los Decision Tree. Esto se
debe a que éste es un modelo muy inestable, y una pequeña alteración en los
datos puede producir resultados muy distintos. Estas condiciones son idoneas
para hacer un comité de Decision Trees, entrenarlos con datos ligeramente
distintos y elegir la solución qué más árboles hayan predicho.

Pero este procedimiento no tiene ningún sentido hacerlo con modelos que no son
inestables. Si los modelos no son inestables, la mayoría de los estimadores
responderán la misma solución, y no servirá para nada haber entrenado tantos
modelos distintos. Es como si en un comité de expertos todos ellos opinaran
igual: para eso no necesitamos todo un comité, con un solo experto nos habría
bastado.

La técnica de \eng{bagging} utiliza el \eng{bootstrap} para generar datasets
distintos para cada uno de los estimadores. Consiste en hacer un remuestreo de
los datos con repetición para cada uno de los estimadores. Esta diferenciación
que se produce es suficiente para los Decision Tree, pero es demasiado leve con
los métodos más estables, como Logit y LinearSVC.

Pero los RFF y Nystroem abren una nueva puerta. Puesto que son aproximaciones
aleatorias de un conjunto infinito, podemos sacar tantos mapeos distintos
como queramos de los datos originales, y por lo tanto podemos diferenciar
todavía más los datasets generados para cada uno de los estimadores.

Además de todo esto, hay una ventaja adicional: entrenar una \eng{Support
Vector Machine} (SVM) con kernel lineal es más barato que entrenar una no lineal, por
ejemplo una que use RBF. Si usamos una SVM lineal, pero en vez de entrenarla
con los datos originales la entrenamos con los datos $\psi(\vx)$, tenemos un
coste similar al de entrenar una SVM lineal pero con una precisión equiparable
a una RBF-SVM. Esto ya se ha hecho antes.

Existen varias formas de combinar las RFF con los métodos ensembles. Básicamente,
hay dos parámetros que podemos elegir: qué tipo de ensemble usar y en qué
momento usar las RFF.

Cuando se combina un ensemble con los RFF, básicamente hay dos momentos en
los que se puede usar el mapeo. Un momento es nada más empezar, antes de
que el ensemble haya visto los datos, y el ensemble trabaja normalmente, solo
que en vez de recibir los datos originales recibe un mapeo de los mismos.
Este método se abstrae completamente de lo que hace el ensemble, y lo trata
como una caja negra. \eng{Black Box}.

El otro método consiste en usar el RFF, no nada más empezar y el mismo para
todos los estimadores, sino justo antes del estimador: se hace un mapeo nuevo
para cada uno de los estimadores. Este método ya se mete dentro de lo que es
un ensemble, y por tanto diremos que es de caja blanca (\eng{White Box}).

Pero se sabe que se obtienen mejores resultados cuando hay bastante diversidad
entre los estimadores del ensemble. Se nos presentan ahora dos formas de crear
diversidad en el ensemble. Una de ellas es la forma clásica, mediante el
bootstrap, que ha mostrado muy buenos resultados con el \eng{Decision Tree}.
Pero ahora podemos usar también la aleatoriedad de los RFF para generar
esa diversidad. Entonces tenemos dos opciones: usar los RFF ellos solos o usarlos
junto con el Bootstrap. A usarlos junto con el bootstrap le llamaremos un
\eng{Bagging}, mientras que si no usamos bootstrap le llamaemos un \eng{Ensemble}.

Tenemos entonces varias combinaciones entre manos:

\paragraph{Black Bag}
Black Box model con Bagging. Primero se hace un mapeo de los datos y después se
hace un bootstrap con ellos para cada uno de los estimadores. Si los estimadores
son \eng{Decision Tree} es los mismo que un \eng{Random Forest}, pero no con los
datos originales, sino con el mapeo.

\paragraph{White Bag}
White Box model con Bagging. Primero se hace un bootstrap de los datos, para
cada uno de los modelos, y después para cada uno de ellos se hace un mapeo de
los datos.

\paragraph{White Ensemble}
White Box model sin baging. Se hace un mapeo para cada uno de los estimadores,
todos ellos usando todos los datos originales.


El \eng{Black Ensemble} no tiene ningún sentido hacerlo, porque en ese caso
todos los estimadores recibirían exactamente los mismos datos, y por lo tanto
todos producirían exactamente los mismos resutados, a no ser que tuvieran algún
tipo de aleatoriedad, como los Decision Tree. A pesar de que tengamos ese caso
particular con los DT, no lo vamos a tratar.

Y luego, por supuesto, haremos pruebas con un modelo simple usando los RFF, sin
usar ningún tipo de ensemble.

\subsubsection{Hipótesis}
De precisión:
\begin{itemize}
  \item Usar bootstrap con RFF es demasiada aleatoriedad y producirá peores
  resultados que usar RFF ellos solos.
  \item
\end{itemize}

De tiempos:
\begin{itemize}
  \item Podemos aproximar la precisión que tendría una RBF-SVM usando una SVM
  lineal con el truco de las RFF con un tiempo mucho mejor
\end{itemize}

\subsection{Los datasets}
He enfocado el trabajo únicamente con problemas de clasificación. He hecho
pruebas con 8 datasets distintos.

Todos ellos los he normalizado a media 0 y varianza 1, y he usado dos tercios
para train y un tercio para test.

\subsubsection{Pen Digits}
\cite[Vease][]{pen_digits}
Distinguir entre los 10 dígitos (0-9) de un conjunto de imágenes. El dataset
se ha generado cogiendo las coordenadas $x$ e $y$ del trazo hecho por una
persona para dibujar ese número e interpolando 8 puntos normalmente espaciados
en todo el trazo del dibujo.


\subsubsection{Covertype}
\subsubsection{Satellite}
\subsubsection{Vowel}
\subsubsection{Fall Detection}
\subsubsection{MNIST}
\subsubsection{Segment}
\subsubsection{Digits}






% El kernel RBF se utiliza mucho en Machine Learning, especialmente en SVM, donde
% lo único importante es el producto escalar entre los vectores de datos. Este
% kernel realiza una transformación implícita de los datos a un espacio de
% dimensionalidad infinita en el que, en el caso de las SVM, se espera que los
% datos sean separables con un híper-plano.
%
% Este nuevo espacio de dimensionalidad infinita

\subsection{State of the art con las RFF}



\section{Experimental results}
\section{Conclussion}
De momento, parece que algunos problemas sí que se benefician de esto, mientas
que otros no lo hacen

\section{Future work}
\begin{itemize}
 \item El trabajo se ha centrado en problemas de clasificación, pero no hay
       ningún motivo para que no se pueda aplicar el regresión. Se ha omitodo por
       simplificar
 \item Aquella teoría de que quizá se puede regular la cantidad de aleatoriedad
       que añade el bootstrap, y quizá inventar un bootstrap con un parámetro para
       regular la cantidad de aleatoriedad
 \item Pensar en aquella teoría de que quizá se puede inventar un procedimiento
       para, dato un problema determinado con sus datos, sacar un número que sea
       representativo de la cantidad promedio de ruido que tiene. Puesto que quizá
       es útil para este proyecto conocer la cantidad de aleatoriedad que tienen
       los datos, para que se pueda regular
\end{itemize}
\section{Sustainability Report}

\input{sustainability_report}

% Aquí empieza el índice que tenía en la fita de seguiment
% \section{Context}
%     \subsection{General Framework}
%     \subsection{Into the specifics}
%     \subsection{State of the Art}
%     \subsection{Problem to solve}
%
% \section{Planning}
%     \subsection{Original Planning}
%     \subsection{Problems encountered with original planning}
%     \subsection{Proposed new planning}
%
% \section{Methodology}
%     \subsection{Original Proposed Methodology}
%     \subsection{Problems encountered with original methodology}
%     \subsection{New methodology}
%
% \section{Alternatives Analysis}
%     \subsection{Language for development}
%     \subsection{Running environment}
%     \subsection{Machine Learning Algorithms}
%
% \section{Knowledge Integration}
%
% \section{Implication and Decision Making}
%     \subsection{Meetings with director}
%     \subsection{Goals achievement}
%     \subsection{Rigour in scientific procedures}
%
% \section{Laws and regulations}
%     \subsection{My responsibility}
%     \subsection{Others responsibility}
%     Esto es algo que hizo \cite{dirac} y también \cite{einstein}



\printbibliography
\end{document}
