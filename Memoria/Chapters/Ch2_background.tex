% Chapter Template

\chapter{Background Information and Theory} % Main chapter title

\label{Chapter2} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Machine Learning}
% \begin{note}
%   \begin{itemize}
%     \item Una definición rápida
%     \item Clasificación y regresión
%     \item Cross-validation
%     \item Qué son los datos de train y test, y por qué se hace esa partición
%     \item Qué es el sobre-ajuste
%   \end{itemize}
% \end{note}

\begin{pre-delivery}
  % Machine Learning uses statistical and mathematical models to give answers to
  % problems when there is no known formula of procedure to compute the answer.

  Machine Learning uses statistical and mathematical models to give
  computational answers based on data to problems when there is no known
  formula of procedure.

  In the subfield of Supervised Learning, the objective is to predict a numerical
  or categorical variable in response to some input data, and the way of doing
  it is to feed the model with lots of different examples for which we already
  know the correct answer, and we expect the models to be able to predict
  the correct answer to instances that it hasn't seen before. When it does,
  we say that the model is able to generalize.

  When a model is trained with some data, there is always a risk of overfitting
  \cite{hawkins2004problem}.
  For a model to overfit means that it adjusts very well to the data that is
  has seen, but can't predict the correct answer to new, unseen data.
  % This
  % happens because it has not only from the relevant information, but also
  % from the random noise that the data had, and so it can only memorise, but
  % not generalize.
  This happens because it has not only fitted the relevant information,
  but also random noise present in the data sample, and thus it generalizes
  poorly. In the extreme case of ovefitting, the model tends to memorise the
  data sample.

  For this reason, when a Machine Learning algorithm is trained the data
  is split in two subsets, a \textit{Training  dataset} and a \textit{Testing
  dataset}. The training dataset will be used to train the model, while the
  Testing datasets will be used only to check it. If a model has generalized
  well, it will achieve a good accuracy score on both the training and the
  testing dataset, but if it has overfitted it will show good results in the
  training dataset and bad ones in the testing dataset.

  Many models need some parameters to tune the behaviour of the algorithm. For
  example, some of them are used to adjust how much a model will fit to the data.
  We usually call these ``hyperparameters''. The correct value for them is not
  straightforward, and it is normally chosen with a resampling process called
  ``cross-validation''\cite{geisser2017predictive}. This process consists of
  splitting the training dataset
  in many subsets and check many possible values for the hyperparameters in order
  to see which one gets a higher accuracy with unseen data.
\end{pre-delivery}

\section{Some currently used Machine Learning models}


% \begin{note}
  % \section{Review de los principales modelos que existen}
% \end{note}
  \subsection{Decision Tree}
  \label{sec:dec-tree}
  % \begin{note}
  %   \begin{itemize}
  %     \item No se basa en productos escalares
  %     \item Es extremadamente rápido
  %     \item Es más fácil de interpretar que otros modelos
  %     \item Es extremadamente inestable
  %     \item Cuando se hace un Random Forest, se randommiza un poco, de modo que
  %     árboles distintos entrenados con los mismos datos pueden ser destintos
  %     \item Es un modelo no lineal
  %   \end{itemize}
  % \end{note}
  \begin{pre-delivery}
    Decision Tree\cite{breiman2017classification}\cite{lewis2000introduction}
    is a predictive model which uses
    the training data to build
    a tree where each node splits the data in two sets according to some
    feature, and the leafs contain the set of instances that belong to some class
    (in classification problems) or that has a similar numerical response variable
    (for regression problems).

    To predict the answer to a new instance, it uses the features to ``decide''
    the nodes to cross until it reaches a leaf. The response given is the
    most prevailing class in the leaf for classification problems, or the mean
    of the values of the rest of the instances in the leaf.

    To decide what feature to use to split a node in two subsets, it uses
    the Gini impurity: it will pick the feature that minimises the sum of
    the Gini impurity of the two child nodes. Given a node with instances
    belonging to $k$ classes, if $p_i$ is the proportion of instances that
    belong to class $i$, the Gini impurity of the node is
    $1 - \sum_{i = 1}^k p_i^2$.

    Decision Trees have the advantages that it is easy to interpret the
    tree produced and that it is very fast to build the tree. The way to avoid
    overfitting is to limit its growth.

    These models are very unstable. This means that small differences in the
    training data can produce very different Decision Trees. This property
    is very useful to build an ensemble of estimators to produce better answers.
    Random Forest is an algorithm that trains many Decision Trees with some sort
    of randomization.
  \end{pre-delivery}
  \subsection{Logistic Regression}
  \label{ssec:log-reg}
  \begin{pre-delivery}
    Logistic Regression\cite{cox1958regression} models the probability that an instance belongs to
    a class, and predicts the class with a higher probability. To do so it
    uses the \textit{logistic sigmoid function}\cite{han1995influence}, defined by:

    \begin{equation}
      \sigma(a) = \frac{1}{1 + exp(-a)}
    \end{equation}

     Once the vector $w \in \reals^d$ has been found, the predicted probability
     that an instance $\vx \in \reals^d$ belongs to a class is
     $y(\vx) = \sigma(w^\transp\vx)$.

     % To find a suitable $w$ it solves an optimization problem of finding $w$
     % that maximizes the likelihood that each of the instances belongs to the
     % specified class

     Given
     $D = \{\bm{\chi}, \bm{t}\}$
     , where
     $\bm{\chi} = \{\bm{x}_1, \ldots \bm{x}_n\}$, $\bm{x}_i \in \reals^d$, $\bm{t} = \{0, 1\}^n$
     it tries to maximize a likelihood function that can be written

     \begin{equation}
       p(\bm{t} | w) = \prod_{i = 1}^n y_i^{\bm{t}_i} (1 - y_i)^{1 - \bm{t}_i}
     \end{equation}

     where $y_i = \sigma(w^\transp\vx_i + w_0)$.
     % , the problem is to find $w \in \reals^d$ and $c \in \reals$ that minimizes

     % \begin{equation}
     %   \frac{1}{2}w^\transp w + \sum_{i = 1}^n log\left( exp\left( -y_i(\vx_i^\transp w + c) \right) + 1\right)
     % \end{equation}
  \end{pre-delivery}
  \subsection{Support Vector Machines}
  \label{ssec:svm}
  % \begin{note}
  %   \begin{itemize}
  %     \item Inicialmente pensadas para clasificación en 2 clases
  %     \item Pero se puede más clases con \eng{one-vs-rest} y también hay
  %     formas de hacer regresión
  %     \item Se basa únicamente en el producto escalar de sus entradas
  %     \item Intenta separar los datos con un híper-plano
  %     \item Actualmente es poco eficiente usarlas porque su coste s cúbico
  %     con la cantidad de entradas.
  %     \item Las fórmulas que quiere optimizar
  %   \end{itemize}
  % \end{note}

  \begin{pre-delivery}
    Support Vector Machine\cite{Cortes1995} (SVM) is a model that finds in hyperplane that
    divides the data in two sets. In two-class classification problems, each
    side of the hyperplane contains the instances of each of the classes.
    It does so by converting the problem to an optimization one.

    Given some data
    $D = \{\bm{\chi}, \bm{y}\}$
    , where
    $\bm{\chi} = \{\bm{x}_1, \ldots \bm{x}_n\}$, $\bm{x}_i \in \reals^d$, $\bm{y} = \{-1, +1\}^n$
    , the optimization problem consists on finding $\bm{\alpha} \in \reals^n$
    the maximises

\begin{equation}
  L = \sum_{i = 1}^n\alpha_i -\frac{1}{2}\sum_{i = 1}^n\sum_{j = 1}^n\alpha_i\alpha_jy_iy_j\vx_i^\transp\vx_j
\end{equation}

subject to

\begin{align}
  0 \leq \alpha_i \leq C; \forall i\\
  \sum_{i = 1}^n \alpha_iy_i = 0
\end{align}

% $C$ is an hyperparameter to tune the amount of penalization for miss-classified
% instances.

$C$ is an hyper-parameter to tune the amount of penalization for missclassified
instances or instances located within the margin zone.

If we compute
\begin{equation}
  w = \sum_{i = 1}^n\alpha_iy_i\vx_i
\end{equation}
and

\begin{equation}
  b = y_i - w\cdot\vx_i
\end{equation}
for any $i$ so that $\alpha_i \neq 0$, we can compute the class of $\vx_0$ with

\begin{equation}
sign(w\cdot\vx_0 + b)
\end{equation}

Note that this algorithm just uses the dot product of the input data, not the
data itself. This property allows us to use the Kernel Trick with them.
See \ref{sec:kern-trick}

\end{pre-delivery}


\section{Ensemble Methods}
\label{sec:ens-meth}
  % \subsection{Bagging}
    % \begin{note}
    %   \begin{itemize}
    %     \item Bagging
    %     \begin{itemize}
    %       \item Inventado por Leo Breiman (referencia)
    %       \item Pretende reducir el sesgo
    %       \item Wikipedia dice que pretende reducir la varianza
    %       \item Es el boosting el que pretende reducir el sesgo
    %       \item Entrenamiento de los estimadores es independiente, se podría
    %       hacer en paralelo
    %       \item Actualmente casi solo se usa con DT, debido a su inestabilidad
    %     \end{itemize}
    %     \item Bootstrap
    %     \begin{itemize}
    %       \item Intenta solucionar el problema de que para bagging es bueno
    %       que los estimadores sean distintos
    %       \item Idealmente usaríamos un dataset distinto para cada estimador
    %       \item Consiste en hacer un resalmpling con repetición
    %       \item Si la cantidad de instancias del original es la misma que la de cada uno
    %       de los subconjuntos, se espera que la proporción de elementos úncos sea de
    %       $1 - \frac{1}{e} \approx 0.632$.
    %       \item Si el conjunto original tiene $n$ elementos, y tu haces un subconjunto
    %       de tamaño $r$, puedes esperar que la proporción de elementos del original que
    %       sí tienen presencia en el nuevo sea de $1 - e^{-\frac{r}{n}}$
    %     \end{itemize}
    %     \item Random Forest
    %   \end{itemize}
    % \end{note}

  \begin{pre-delivery}
    Ensemble methods\cite{polikar2006ensemble} are a technique used in Machine
    Learning to reduce the
    overall accuracy error of a basic classification or regression model. The
    idea is that a commimtee of models is expected to learn better than a single
    one.

    Some ensemble methods are focused on decreasing the error caused by the
    variance of the model. One example is \textit{Bagging}\cite{breiman1996bagging}. Others are focused
    on decreasing the bias error, like \textit{Boosting}\cite{freund1997decision}.

    In Bagging, every model in the ensemble vote with equal weight. Thus, it is
    important to promote the variance among each of the models, since not doing
    it would be equivalent to training just one model. Ideally, one would train
    each of the models with totally different datasets, with no correlation
    among them. But in practice this is not always possible, because of a
    limited number of instances to train. One alternative is to use a
    technique called \textit{Bootstrap}\cite{efron1994introduction}. Bootstrap allows to generate
    many different instances of a dataset by performing a resampling.

    Given a dataset $D$ of size $n$, Bootstrap generates $m$ new datasets
    $D_i$ of size $n$ by sampling instances from $D$ uniformly and with
    replacement. This means that some of the instances in $D$ may be repeated
    in $D_i$, and others may not appear at all. With a large $n$, it is expected
    that each dataset $D_i$ will contain $63.2 \% $ of the instances in $D$.

    Theoretically Bagging could be used with any kind of method. However, for
    most of them Bootstrap is not enough to decorrelate the estimators.
    In practice, Bagging is mostly used with Decision Tree, given that this
    method produces very different trees with a small variation in the data.
    Random Forest\cite{Breiman2001} is an algorithm that trains many Decision Trees with a
    Bagging. Instead of building the tree in a deterministic way, in each
    split it chooses a random subset of features on which to perform the
    separation. Besides, it lets the estimators overfit, since it has a positive
    impact in reducing the overall variance of the Forest.
  \end{pre-delivery}

\section{The kernel trick}
\label{sec:kern-trick}




% \begin{note}
%   \begin{itemize}
%     \item Teorema de Bochner
%     \item El kernel RBF
%     \begin{itemize}
%       \item Su fórmula es \ldots
%       \item Equivalencia entre $\gamma$ y $\sigma$
%       \item La noción de similitud que tiene
%       \item \Hspace\ es de dimensionalidad infinita
%       \item Permite ajustarse infinitamente a los datos, tuneando el
%       híper-parámetro
%       \item $\sigma$ más pequeño, más sobreajuste
%       \item $\gamma$ más grande, más sobreajuste
%     \end{itemize}
%   \end{itemize}
% \end{note}

\begin{pre-delivery}
  A Kernel\cite{bergman1970kernel} is a function that equals to the inner product of inputs mapped into
  some Hilbert Space
  \footnote{A Hilbert space is a generalization of the Euclidean Space which contains
  the structure of an inner product that allows length and angle to be
  measured.}
  , i.e:
  \begin{equation}
  \kernel(x,y) = \phi(x)\cdot\phi(y)
\end{equation}
% A Hilbert space is just a generalization of the Euclidean Space which contains
% the structure of an inner product that allows length and angle to be measured.

They are interesting in Machine Learning because we don't need to know the
explicit function $\phi(\cdot)$. In fact, $\phi(\cdot)$ could map the data to
a Hilbert Space with infinite dimensions, and we could still compute
$\phi(\vx)\cdot\phi(\vy)$ through the kernel $\kernel$

Support Vector Machines (explained in \ref{ssec:svm}) can benefit a lot of
Kernel Functions. SVMs solve an
optimization problem to maximise

\begin{equation}
  L = \sum_{i = 1}^n\alpha_i -\frac{1}{2}\sum_{i = 1}^n\sum_{j = 1}^n\alpha_i\alpha_jy_iy_j\vx_i^\transp\vx_j
\end{equation}

in order to find an hyperplane that separates the data points in two classes.
But with some problems there may not exist such hyperplane, and so it would
be needed to map the data to a different feature space. If we did that, then
the function to maximise would be

\begin{equation}
  L = \sum_{i = 1}^n\alpha_i -\frac{1}{2}\sum_{i = 1}^n\sum_{j = 1}^n\alpha_i\alpha_jy_iy_j\phi(\vx_i)^\transp\phi(\vx_j)
\end{equation}

As we said previously, SVMs don't work with the data points alone, but just with
their inner products. Thus, a Kernel could be used to define the optimization
problem as

\begin{equation}
  L = \sum_{i = 1}^n\alpha_i -\frac{1}{2}\sum_{i = 1}^n\sum_{j = 1}^n\alpha_i\alpha_jy_iy_j\kernel(\vx_i, \vx_j)
\end{equation}

This approach has one big advantage:
% first, we don't need to explicitly
% compute $\phi(\vx)^\transp\phi(\vy)$, which could have a high cost if the
% new dimensionality was too big.
as long as the learning technique relies
only on the inner product of the input, the underlying mapping $\phi(\cdot)$
does not need to be explicitly calculated and can, in fact, be unknown\cite{burges1998tutorial}.

Kernel functions can be characterised with the Mercer's condition
\cite{mercer1909functions}. It says that given a function $\kernel(\vx, \vy)$,
there exists a mapping $\phi(\cdot)$ so that
$\kernel(\vx, \vy) = \phi(\vx)\cdot\phi(\vy)$
if and only if for any $g(\vx)$ such that $\int g(\vx)^2 d\vx$ is finite then
$\int \kernel(\vx, \vy)g(\vx)g(\vy) \geq 0$.

There are many known Kernels. One that is very popular is the Radial Basis
Function Kernel\cite{vert2004primer}, RBF. This kernel is defined as:
\begin{equation}
\kernel(\vx,\vy) = \semiRbf
\end{equation}
where $\gamma > 0$ is a free parameter. The value of this Kernel decreases with the
euclidean distance of the parameters, so it can be interpreted as a measure
of similarity. The feature space of this kernel has infinite number of
dimensions.

When a kernel is used with an SVM, the answer can be computed with

\begin{equation}
sign\left(\sum_{i = 1}^n \alpha_iy_i\kernel(\vx_i, \vx)\right)
\end{equation}
SVMs using the RBF kernel have a huge ability to fit to the data, and is able
to separate classes for very difficult problems. The problem is that the
optimization of the function

\end{pre-delivery}
  % \subsection{The RBF kernel}

\section{Random Fourier Features}

% \begin{note}
%   \begin{itemize}
%     \item Teorema de bochner
%     \item Tiene que ser un shift invariant kernel
%     \item Es más, tiene que ser un positive definite shift-invariant kernel
%     \item Converge bounds for ability to approximate
%     \item Instead, we propose to factor the kernel function itself
%     \item La factorización no depende de los datos
%     \item we propose explicitly mapping
% the data to a low-dimensional Euclidean inner product space using a randomized feature map z :
% Rd --> RD so that the inner product between a pair of transformed points approximates their kernel
% evaluation
% \item Puesto que los valores están entre -1 y 1, hay un teorema que asegura
% la convergencia exponencial hacia el kernel real
%   \end{itemize}
% \end{note}

\begin{pre-delivery}
  A kernel function
  $\kernel(\vx, \vy)$ with $\vx, \vy \in \reals^d$
  equals the inner product of inputs mapped with some function $\phi(\cdot)$,
  so that
  $\phi(\vx)^\transp\cdot\phi(\vy) = \kernel(\vx, \vy)$.
  But $\phi(\cdot)$ could be a mapping to an infinitely-dimensional space, so
  calculating $\phi(\vx)$ is not possible for some kernels.

  Random Fourier Features\cite{rahimi2008random} provide a way to, given a
  kernel $\kernel(\vx, \vy)$,
  explicitly map the data to a
  low-dimensional Euclidean inner product space using a randomized feature
  map $z: \reals^d \mapsto \reals^D$ so that the inner product between a pair
  of transformed points approximates their kernel evaluation, i.e:

  \begin{equation}
    \kernel(\vx, \vy) = \phi(\vx)^\transp\cdot\phi(\vy) \approx z(\vx)^\transp\cdot z(\vy)
  \end{equation}

  To approximate the RBF kernel, it uses the Bochner's Theorem, which says:

\newtheorem{theorem}{Theorem}
  \begin{theorem}
    \cite{rudin1962fourier}
    A continuous kernel $\kernel(x, y) = \kernel(x - y)$ on $\reals^D$  is
    positive definite if and only if $k(\delta)$ is the
    Fourier Transform of a non-negative measure.
  \end{theorem}

  % Since RBF is defined as $\kernel(\vx, \vy) = e^{-\gamma\norm{\vx - \vy}^2}$

  Since it is known that RBF is shift-invariant and positive definite, then
  its Fourier transform is a proper probability distribution, and so

  \begin{equation}
    \kernel(\vx - \vy) = \int_{\reals^D} p(w)e^{iw^\transp (\vx-\vy)}
    = \int_{\reals^D} p(w)cos\left(w^\transp (\vx-\vy)\right)
  \end{equation}

  A random feature can be obtained by picking $w \sim \{\mathcal{N}(0, 2\gamma)\}^d$
  and $b \sim \mathcal{U}(0, 2\pi)$
  and computing $\sqrt{2}cos(w^\transp\vx + b)$. To generate a lower variance
  approximation of $\phi(\vx)$ with $D$ features we can concatenate $D$ randomly
  chosen features $(f_1, \ldots, f_D)$ into a column vector and normalize each
  component by $\sqrt{D}$.

  It is guaranteed an exponentially fast convergence in $D$ between
  $z(\vx)^\transp z(\vy)$ and $\kernel(\vx, \vy)$.

\end{pre-delivery}

\section{\Nys}

\begin{pre-delivery}
  The \Nys\cite{NIPS2000_1866} method is a general method for low-rank approximations of
  kernels. It achieves this by subsampling the data on which the kernel
  is evaluated.

  In kernel methods the data can be represented in a kernel matrix $K$, where
  $K_{i,j} = \kernel(\vx_i, \vx_j)$. The problem of these methods is their
  high computational cost associated with the kernel matrix: with non-linear
  kernels, the cost of training the model is cubic with the number of
  instances, something unacceptable for large-scale problems.

  The \Nys\ method consists on generating an approximation of the kernel matrix of
  ranq $q$, where $q$ can be a lot smaller than the number of instances, without
  any significant decrease in the accuracy of the solution. This way, if there
  are $n$ instances in a dataset, the complexity can be reduced from
  $\mathcal{O}(n^3)$ to $\mathcal{O}(nq^2)$.

  With \Nys, given a kernel $\kernel(\vx, \vy) = \phi(\vx)\cdot\phi(\vy)$, one can
  construct a mapping $z: \reals^d \mapsto \reals^q$ so that
  $z(\vx) \approx \phi(\vx)$. This function defines each component $j$ as
  $z_j(\vy) = \frac{1}{q}\sum_{i = 1}^q \kernel(\vy, \vx_i)g_i(\vx_i)$,
  where $\vx_1, \ldots, \vx_q$ are some chosen instances and
  $g_i(\cdot)$ comes from a column from the Singular Value Decomposition
  of the approximated kernel matrix.
\end{pre-delivery}
