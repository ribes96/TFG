% Chapter Template

\chapter{Background Information and Theory} % Main chapter title

\label{Chapter2} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Machine Learning}
\begin{note}
  \begin{itemize}
    \item Una definición rápida
    \item Clasificación y regresión
    \item Cross-validation
    \item Qué son los datos de train y test, y por qué se hace esa partición
    \item Qué es el sobre-ajuste
  \end{itemize}
\end{note}

\begin{note}
  \section{Review de los principales modelos que existen}
\end{note}
  \subsection{Decision Tree}
  \begin{note}
    \begin{itemize}
      \item No se basa en productos escalares
      \item Es extremadamente rápido
      \item Es más fácil de interpretar que otros modelos
      \item Es extremadamente inestable
      \item Cuando se hace un Random Forest, se randommiza un poco, de modo que
      árboles distintos entrenados con los mismos datos pueden ser destintos
      \item Es un modelo no lineal
    \end{itemize}
  \end{note}
  \subsection{Logistic Regression}
  \begin{delivery}
    Hola que tal
  \end{delivery}
  \subsection{Support Vector Machines}
  \begin{note}
    \begin{itemize}
      \item Inicialmente pensadas para clasificación en 2 clases
      \item Pero se puede más clases con \eng{one-vs-rest} y también hay
      formas de hacer regresión
      \item Se basa únicamente en el producto escalar de sus entradas
      \item Intenta separar los datos con un híper-plano
      \item Actualmente es poco eficiente usarlas porque su coste s cúbico
      con la cantidad de entradas.
      \item Las fórmulas que quiere optimizar
    \end{itemize}
  \end{note}

\section{Ensemble Methods}
  % \subsection{Bagging}
    \begin{note}
      \begin{itemize}
        \item Bagging
        \begin{itemize}
          \item Inventado por Leo Breiman (referencia)
          \item Pretende reducir el sesgo
          \item Entrenamiento de los estimadores es independiente, se podría
          hacer en paralelo
          \item Actualmente casi solo se usa con DT, debido a su inestabilidad
        \end{itemize}
        \item Bootstrap
        \begin{itemize}
          \item Intenta solucionar el problema de que para bagging es bueno
          que los estimadores sean distintos
          \item Idealmente usaríamos un dataset distinto para cada estimador
          \item Consiste en hacer un resalmpling con repetición
          \item Si la cantidad de instancias del original es la misma que la de cada uno
          de los subconjuntos, se espera que la proporción de elementos úncos sea de
          $1 - \frac{1}{e} \approx 0.632$.
          \item Si el conjunto original tiene $n$ elementos, y tu haces un subconjunto
          de tamaño $r$, puedes esperar que la proporción de elementos del original que
          sí tienen presencia en el nuevo sea de $1 - e^{-\frac{r}{n}}$
        \end{itemize}
        \item Random Forest
      \end{itemize}
    \end{note}

\section{The kernel trick}
\begin{note}
  \begin{itemize}
    \item Teorema de Bochner
    \item El kernel RBF
    \begin{itemize}
      \item Su fórmula es \ldots
      \item Equivalencia entre $\gamma$ y $\sigma$
      \item La noción de similitud que tiene
      \item \Hspace\ es de dimensionalidad infinita
      \item Permite ajustarse infinitamente a los datos, tuneando el
      híper-parámetro
      \item $\sigma$ más pequeño, más sobreajuste
      \item $\gamma$ más grande, más sobreajuste
    \end{itemize}
  \end{itemize}
\end{note}
  % \subsection{The RBF kernel}

\section{Random Fourier Features}
\section{\Nys}
