% Chapter Template

\chapter{Project Development} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{General Idea}
% \begin{note}
%   \begin{itemize}
%     \item Hemos visto que se puede sacar una aproximación aleatoria de la
%     función implícita de un shift invariant kernel. Esto tiene 2 ventajas
%     \begin{itemize}
%       \item Podemos transformar los datos directamente
%       \item Podemos producir pequeñas variaciones de un mismo dataset, todas
%       ellas válidas
%     \end{itemize}
%     \item Las 4 tipos de modelos que he definido. Referencia a la foto
%     \item ¿Por qué he cogido estos 4 modelos? ¿No podrían haber sido otros?
%     ¿Que tienen estos de bueno? Me he inspirado en Random Forest
%     \item Hay por ahí algún paper que compara RFF y \Nys
%   \end{itemize}
% \end{note}

% \begin{note}
%   \begin{itemize}
%     \item Usar RFF añade un coste al procedimiento, pero es lineal con la
%     cantidd de instsancias
%     \item Por lo tanto, el uso de las RFF probablemente solo será útil con
%     datasets muy grandes. Si el dataset es pequeño, los costes que tiene son
%     demasiado grandes
%     \item De los 9 problemas con los que hemos experimentado, 7 de ellos son
%     pequeños y únicamente sirven para ilustrar la mejoría de precisión, pero
%     no los tiempos
%     \item El MNIST y el Fashion MNIST se han usado para sacar conclusiones de
%     tiempo
%   \end{itemize}
% \end{note}
\begin{pre-delivery}
  RFF and \Nys\ can be very useful for two main reasons:
  \begin{itemize}
    \item They allow us to perform an explicit mapping approximating the Kernel
    feature space.
    \item They can be used to generate many different equally valid datasets
    with the required number of features from a single one.
  \end{itemize}

  We will study how these advantages could be used to increase the accuracy
  of some models. In particular, we expect they will be useful in two
  different ways. First, some methods could achieve a higher accuracy if they
  were trained with data in a Kernel feature space instead of the general
  one. Second, generating many datasets could make it possible to increase the
  accuracy of some models by training an ensemble.

  Although both approaches are expected to increase the accuracy of some
  models, they will also increase a little bit the training time. Performing
  a random mapping has a linear cost with the number of instances, and training
  an ensemble clearly multiplies the ammount of work to be done. Thus, the
  benefit of the methods presented will depend on the circumstances of
  each problem. It should be noted that both increases in time are linear with the
  number of instances, so they can scale well with very large datasets. This is
  in contrast to training SVM with non-linear kernels, whose cost is cubic
  with the number of instances.

  For this project we have used 9 different datasets to check the hypothesis. 7
  of them were quite small (5000 instances approx.) and the other 2 had a much
  bigger size (70000 instances). For this reason, the first ones will usefull
  to compare the accuracies obtained, but will show a very big increase in the
  training time. In contrast, the bigger datasets will show the real strenth
  of these methods.

  We propose two ways of using the random mappings. The first one is very
  straightforward: simply using a random mapping of the data to train a
  single model, and use the same mapping in the prediction step with
  the instances in the input. The second one involves mixing the mapping
  with some ensemble algorithm.

  There are many ways to mix these two methods. The ones that we have studied
  are based on the Random Forest and are the most straightforward, but other
  ways could be studied in a future work.

  The first thing to decide is where to place the random mapping in the whole
  process. We have taken two different approaches: we can think of an
  ensemble as a Black Box, where we can only affect the inputs and the outputs,
  or we can differenciate the different parts it contain, like in a White Box.

  Then, there's what kind of ensemble to use. We've chosen to use the
  Bagging technique, which is the one that Random Forest uses, but we've
  also defined a modified version which doesn't perform the Bootstrap. This is
  because we expected that using Bootstrap together with a random mapping would
  produce too much randomness in the data, affecting negatively the
  overall accuracy. We've called the first method a ``Bag'', and the second
  method an ``Ensemble''.

  From the combination of these approaches we have defined four different
  ways of mixing: ``Black Bag'', ``Black Ensemble'', ``White Bag'' and
  ``White ensemble''. See \ref{fig:model-boxes}.

  For this project we have used Random Fourier Features and the \Nys\
  method to approximate the Radial Basis Function (RBF) Kernel with tree
  well-known Machine Learning algorithms: Support Vector Machine, Logistic
  Regression and Decision Tree. We study how we can use them to increase
  the accuracy that we can achieve with them and the computational costs.


\end{pre-delivery}
% \begin{pre-delivery}
%   Using RFF or \Nys\ has two main advantages related to this project:
%   \begin{itemize}
%     \item We can use an explicit mapping of the data instead of the implicit
%     one defined by the Kernel functions.
%     \item We can produce many different datasets (all of them equally valid)
%     from an original one, with the required number of features.
%   \end{itemize}
%   These advantages allow us to define some combinations of the Bagging
%   Ensemble method with the Random Mappings. Depending on where do we place
%   the Random Mapping in the ensemble process we can get two different approaches.
%   If we understand the ensemble as a black box, on which we can only affect the
%   inputs and the outputs of the box without affecting the rest of the process,
%   we get what we have called the Black Box Model. If, on the contrary, we use
%   the Random Mapping in the middle of the ensemble process, we get a White Box
%   Model.
%
%   But we have defined two other models based on these ones. Given that we have
%   assumed that maybe a Bootstrap in combination to the Random Mapping woud be
%   too much randomness for the problem, we have defined also the models which
%   doest perform the Bootstrap. We've called a ``Bag'' the models which do
%   perform a Bootstrap on the data, and ``Ensemble'' to those that don't
%   perform it. Thus, we have defined four models: ``Black Bag'', ``White Bag'',
%   ``Black Ensemble'', and ``White Ensemble''. See \ref{fig:model-boxes}
%   % See Figures \ref{fig:black-bag},
%   % \ref{fig:black-ens}, \ref{fig:white-bag} and \ref{fig:white-ens}.
%
%
%   Since the Black Ensemble models will feed all the estimators with the same
%   data, it only makes sense to use it with models with some randomness in the
%   process. That's why we will barely use it here.
%
%   These models are based on what is done in Random Forest with the Decision
%   Tree, and seem to be the logical ones to start with. That's why we have chosen
%   to work with them.
% \end{pre-delivery}
\BoxesFigure
% \BlackBoxes
% \figBlackBag
% \figBlackEns
% \figWhiteBag
% \figWhiteEns
% \begin{note}
%   \subsection{State of the art con las RFF}
% \end{note}
% \begin{note}
%   \begin{itemize}
%     \item Se ha trabajado poco con ellas. Solo he encontrado 2 usos:
%     \begin{itemize}
%       \item Stacked kernel network (referencia): usarlas junto a una red
%       neuronal para tener más niveles de aprendizaje no lineal
%       \item RFF with SVM (referencia): usar una SVM sin kernel con los datos
%       mapeados usando RFF
%     \end{itemize}
%   \end{itemize}
% \end{note}
% \begin{note}
%   \subsection{State of the art con las \Nys}
% \end{note}


\section{Hyper-parameters}
% \begin{note}
%   \begin{itemize}
%     \item Existen los siguientes:
%     \begin{itemize}
%       \item min-impurity-decrease para DT
%       \item C para SVM
%       \item gamma para RFF y \Nys
%       \item cantidad de features para RFF y \Nys
%       \item cantidad de estimadores para ensembles
%     \end{itemize}
%     \item Hemos usado los siguientes valores:
%     \begin{itemize}
%       \item Cantidad de features a 500
%       \item Cantidad de estimadores a 50
%       \item En modelos simples, el parámetro por crossvalidation
%       \item En modelos simples con RFF, el parámetro por crossvalidation
%       y una gamma que sobreajuste
%       \item En modelos con ensemble, parámetros que sobreajusten y la gamma
%       por crossvalidation
%       \item En RBF-SVM, la gamma por gamest y el parámetro por crossvalidation
%     \end{itemize}
%   \end{itemize}
% \end{note}

\begin{pre-delivery}
  With the models defined in this project there are many hyper-parameters to
  tune the models. These are the hyper-parameters that have been used in the
  experiments:
  \begin{description}
    \item[Number of features extracted from the kernel] The higher this value
    is, the better the appriximation of the kernel function. We have fixed a
    value of 500, since this is not important as long as it is not too low.
    \item[Ammount of estimators] Having a large number of estimator doen't
    affect negatively the accuracy obteined, but increases the computation
    time, so the ideal number depends on the computational resources
    available. For this project we have picked 50 estimators for each Bag/Ensemble.
    \item[Gamma parameter of the RBF Kernel] A higher value will generate a
    higher overfit. There is a fast method to find a suitable value for this
    parameter, explained in
    \cite{caputo2002appearance}
    % \cite{nys_better_rff}
    . We have chosen this estimation.
    \item[Parameters of the simple models] Decision Tree use
    \textit{min\tu impurity\tu decrease} to tune the overfit, and SVM use a
    penalty \textit{C} to do the same. When we train these models without
    any ensemble, we use Cross-Validation to find a suitable value. When we
    train an ensemble of these models, as we want them to overfit we set
    \textit{min\tu impurity\tu decrease} to 0 and \textit{C} to 1000, whih
    is enoght to achieve it.
  \end{description}
\end{pre-delivery}
\section{Hypothesis}


% \begin{note}
%   \begin{enumerate}
%     \item Podemos aproximar bien una RBF-SVM
%     \item Puede tener sentido hacer ensembles con otros modelos a DT
%     \item RFF + Bootstrap puede ser malo
%     \item Si el modelo no se basa en productos escalares no se
%     feneficiará tanto
%   \end{enumerate}
% \end{note}

% \begin{tcolorbox}[breakable, colback=red,coltext=black]
%   El paper que hacer Linear SVM con RFF no está indexado en ningún sitio. Es
%   solo un pdf que hay por la red
% \end{tcolorbox}
\begin{pre-delivery}
  We had proposed these four hypothesis:
  \begin{enumerate}
    \item \textbf{It is possible to achieve an acuracy close to using the
    RBF Kernel but with a lower cost}

    When the number of instances available is too big it is not possible to
    use an SVM with the RBF kernel, because the cost is cubic with the
    number of instances, and the optimization problem is too complex. A linear
    Kernel needs less time, but it may not be suitable for some problems,
    since data may not be easy to separate.

    If we could first map the data to the new feature space, we could then feed
    a Linear SVM with it and have the same accuracy with less costs. But this
    can't be done with the RBF kernel, since the new feature space has infinite
    dimensions. However, with the use of RFF and \Nys, we can get an approximation
    of the feature space of the RBF. Using them with a Linear SVM could
    increase the accuracy on some datasets at almost the same cost.

    \item \textbf{It could make sense to train ensembles of SVM and Logistic
    Regression algorithms}

    Since these models are very stable, having an ensemble of them is useless:
    all of them will allways predict the same answer. There are some
    methods to randomize a little bit the data, such as Bootstrap, but with
    these models it is not enough.

    Since RFF and \Nys\ generate a random mapping of the data, we can achieve
    a higher level of randomization of the data, while still being a good
    representation of the real data. Random Mapping can allow us to build
    ensembles with these two models, increasing the overall accuracy at the
    expense of some computation time.

    \item \textbf{Bootstrap together with a Random Mapping may be too much
    randomization}

    With a simple mix of Bagging with RFF there are two different sources
    of randomness. For the one hand, Bootstrap generates a random sample of
    the data with replacement, and on the other hand, RFF and \Nys\ perform
    a Random Mapping of the data to a different feature space.

    It is possible that for some models, this is too much randomization of
    the data, and it coud have a bad effect on the learning process.

    \item \textbf{Decision Tree does not benefit from RFF and \Nys\ as much as
    Logistic Regression and SVM do}

    Kernels were originally used on Support Vector Machines because they were
    a fast way to implicitly compute the inner product of two vectors in a
    feature space where data was separable by an hyper-plane. They were
    useful because SVM just needed the inner products of their input to work.

    RFF and \Nys\ are ways to explicitly compute an approximation of that
    mapping, which doesn't necessarily fits the requirements of Decision Tree,
    which has nothing to do with the inned products. That's the reason why
    Decision Tree may not benefit so much of these Random Mappings.
  \end{enumerate}
\end{pre-delivery}
% \begin{note}
%   \subsection{Planteamiento de los experimentos}
% \end{note}
\subsection{Experiments Proposal}
\begin{note}
  \begin{enumerate}
    \item Hipótesis: Aproximar RBF-SVM
    \begin{enumerate}
      \item Comparar una RBF-SVM con SVM normal que use RFF
    \end{enumerate}
    \item Hipótesis: Ensembles con otros
    \begin{enumerate}
      \item Logit normal vs. Logit con RFF
      \item Logit normal vs. Logit con RFF Black Bag
      \item Logit normal vs. Logit con RFF Grey Bag
      \item Logit normal vs. Logit con RFF Grey Ensemble
      \hrule
      \item Linear-SVM vs Linear-SVM con RFF
      \item Linear-SVM vs Linear-SVM con RFF Black Bag
      \item Linear-SVM vs Linear-SVM con RFF Grey Bag
      \item Linear-SVM vs Linear-SVM con RFF Grey Ensemble
    \end{enumerate}
    \item Hipótesis: RFF + Bootstrap
    \begin{enumerate}
      \item Logit con RFF Grey Bag vs Logit con RFF Grey Ensemble
      \item Logit con RFF Black Bag vs Logit con RFF Black Ensemble (los
      dos con un solo estimador)
      \hrule
      \item Linear-SVM con RFF Grey Bag vs Linear-SVM con RFF Grey Ensemble
      \item Linear-SVM con RFF Black Bag vs Linear-SVM con RFF Black Ensemble (los
    \end{enumerate}
    \item Hipótesis: DT + RFF
    \begin{itemize}
      \item DT vs DT con RFF
      \item DT vs DT con RFF Black Bag
      \item DT vs DT con RFF Black Ensemble
      \item DT vs DT con RFF Grey Bag
      \item DT vs DT con RFF Grey Ensemble
    \end{itemize}
  \end{enumerate}
\end{note}

\begin{pre-delivery}

  There are two factors we need to check in order to accept of refute the
  hypothesis: we want to know if these methods increase the accuracy of the
  models (and if so, how much) and also at what costs does it comes, the
  increase of training time. The 7 first datasets (Covertype, Digits, Fall
  Detection, Pen Digits, Satellite, Segment and Vowel) are suitable to measure
  the increase of the accuracy, but not to measure the time, since the
  overhead caused by these techniques is too much compared with training
  with a problem as small as that. The other two datasets (MNIST and Fashion
  MNIST) have a larger number of instances, and are expected to reflect
  better the benefits of using these methods.
  However, some of the models proposed in the experiments are very expensive to
  train on very large datasets, and it was not possible to run the whole training
  with them. Some simplifications have been made to extrapolate the real
  training time.

  To check the hypothesis previously suggested we have proposed a set of
  experiments:

  \subsubsection*{Hypothesis 1}
  In this hypothesis we assumed that we could train an SVM with results comparable
  to the ones using the RBF Kernel, but with much less training time. To
  check that we have defined the experiment 1.1, where we compare four different
  models: an SVM without a kernel, an SVM with the RBF kernel and two SVM
  without kernel but using the random mappings, RFF and \Nys.
  % In order to accept
  To consider that results support
  the hypothesis we need to see that the models using RFF and \Nys\ significantly
  increase the accuracy compared to the single SVM.

  For the method to be useful it would be needed for the training time to
  be much less using the random mappings compared to the RBF-SVM on large datasets.
  To check that we will need to focus on MNIST and Fashion-MNIST, since they
  are the only large datasets.

  Theoretically, for RBF-SVM a cross-validation should have been performed to
  find a suitable value for C, but the training time needed to do that for the
  large datasets is too much. To avoid that, we have used the same value found
  for the SVM with RFF in MNIST and Fashion-MNIST,
  % and have approximated the
  % time that would have been needed if cross-validation had been performed.
  and have multiplied the resulting time by 5 in order to approximately match
  the real required time with cross-validation.

  Results of this experiment can be seen in Appendix \ref{Appendix1-1}


  \subsubsection*{Hypothesis 2}

  For this hypothesis it was asserted that training an ensemble of SVM and
  Logistic Regression using a random mapping could increase significantly the
  accuracy obtained. Nevertheless, the training time is expected to be
  significantly greater since training an ensemble requires a lot more steps.

  Experiments 2.1 through 2.4 were defined to check that. The first two are for
  Logistic Regression and the others for SVM.
  2.1 and 2.3 will compare a single model alone against a single model using
  a random mapping. 2.2 and 2.4 will compare a single model against the model
  with Black Bag, White Bag and White Ensemble. The Black Ensemble model is not
  tested because these estimators are not randomized, and each one would predict
  the same answer.

  For the hypothesis to be supported we need to see that we can achieve an
  increase in the accuracy using any of the ensemble methods proposed. Then we
  will need to study the increase in the training time, and consider if it
  is worth it.

  Results of these experiments can be seen in Appendices
  \ref{Appendix2-1},
  \ref{Appendix2-2},
  \ref{Appendix2-3} and
  \ref{Appendix2-4}.
  \end{pre-delivery}

  % \begin{note}
  %   \begin{itemize}
  %     \item Solo por aclarar alguna idea, incluir también una gráfica que
  %     compare un solo rff con el resto de ensembles, a ver si la mejoría
  %     viene por eso
  %   \end{itemize}
  % \end{note}

  \begin{pre-delivery}
  \subsubsection*{Hypothesis 3}

  With this hypothesis we wanted to know if using Bootstrap together with a
  random mapping will produce worse results than just using the random
  mapping. To check that we will compare the ``Bags'' against the ``Ensembles''.
  Experiments 3.1 and 3.3 will compare ``White Bag'' and ``White ensemble'' for
  Logistic Regression and SVM respectively, and 3.2 and 3.4 will compare
  ``Black Bag'' and ``Black Ensemble''. To make it fairer only one estimator
  has been trained for the ``Black models'' , since ``Black Ensemble''
  doesn't benefit from using more.

  % In order to accept
  To support
  this hypothesis we would need to see a meaningful
  differences in the accuracy of these models.

  Results of these experiments can be seen in Appendices
  \ref{Appendix3-1},
  \ref{Appendix3-2},
  \ref{Appendix3-3} and
  \ref{Appendix3-4}.



  \subsubsection*{Hypothesis 4}

  In this hypothesis we asserted that Decision Tree, which doesn't work with
  the inner product of the inputs, would not experiment a meaningful increase
  in the accuracy with the usage of a random mapping approximating the
  RBF feature space. Decision Tree can normally increase their accuracy
  by using a normal Bagging. That algorithm is called Random Forest. For this
  reason the ensemble methods using a random mapping should be compared agains
  the Random Forest instead of a single Decision Tree.

  Experiment 4.1 compares a single Decision Tree with one that uses a
  random mapping, and 4.2 compares the Random Forest with
  ``Black Bag'',
  ``Black Ensemble'',
  ``White Bag'' and
  ``White Ensemble''.

  To support this hypothesis we should see that using a random mapping doesn't
  outperform too much compared to not using it.

  Results of these experiments can be seen in Appendices \ref{Appendix4-1}
  and \ref{Appendix4-2}
\end{pre-delivery}
% \begin{note}
%   \subsubsection*{Additionally}
%   \begin{itemize}
%     \item Quizá, para los de hipótesis 2 puedo poner gráficas para contrastar
%     RFF con \Nys
%   \end{itemize}
% \end{note}

\section{Datasets}

\begin{table}
 \caption{Information on the datasets used in this project}
 \label{tab:dts-info}
 \centering
 \begin{tabular}{l l l l}
  \toprule
  \tabhead{Dataset}                   & \tabhead{N. Instances} & \tabhead{N. Features} & \tabhead{N. Classes} \\
  \midrule
  Covertype\cite{covertype}           & 4900                   & 12                    & 7                    \\
  Digits\cite{digits}                 & 5000                   & 10                    & 10                   \\
  Fall Detection\cite{fall-detection} & 5000                   & 6                     & 6                    \\
  MNIST\cite{mnist}                   & 70000                   & 784                   & 10                   \\
  Pen Digits\cite{pen-digits}         & 5000                   & 16                    & 10                   \\
  Satellite\cite{satellite}           & 5000                   & 36                    & 6                    \\
  Segment\cite{segment}               & 2310                   & 19                    & 7                    \\
  Vowel\cite{vowel}                   & 990                    & 11                    & 10                   \\
  Fashion MNIST\cite{xiao2017/online}                       & 70000                  & 784                   & 10                   \\
  \bottomrule                                                                                                 \\
 \end{tabular}
\end{table}

% \begin{note}
%   \begin{itemize}
%     \item 8 Datasets
%     \item Normalizados
%     \item Únicamente tienen variables numéricas, no categóricas
%     \item Únicamente problemas de clasificación
%     \item Algunas cosas particulares que he hecho:
%     \begin{itemize}
%       \item Mezclar datos de train y de test para luego hacer mi propia
%       separación
%       \item Cuando había poca presencia de una clase, hacer un resampling para
%       igualar las cantidades
%       \item No trabajar cosas como el skiwness o los outliers
%       \item Eliminar columnas en las que todo eran 0
%       \item Reducir el conjunto de instancias
%     \end{itemize}
%   \end{itemize}
% \end{note}

% % \begin{pre-delivery}
%   \begin{incomplete}
%   We have chosen 9 different datasets to run the experiments proposed. All of
%   them are from classification problems since that was the scope of the project.
%   All the features have been normalized to have a mean of 0 and a variance of 1. Since
%   the kernel RBF can only work with numerical data, all categorical variables
%   have been converted to integers. Here is the preprocessing that
%   has been done on the datasets:
%   \paragraph{Covertype}
%   In the original dataset the classes were not balanced. So, we performed a
%   resampling of the data, taking 700 random instances of each of the clases.
%   There were 40 binary columns that were converted to a single numerical
%   variable.
%   \paragraph{MNIST}
%   We took a subset of 5000 instances from the original dataset. Since in the new
%   sample some of the columns contained only zeros, they were removed.
% % \end{pre-delivery}
% \end{incomplete}


\begin{pre-delivery}
  This project is focused on studying the effects of Random Fourier Features
  and \Nys\ on classification problems, but there is nothing that prevents their
  usage for regression. It is proposed as a future work to perform the
  same experiments on regression problems.

  The RBF kernel is just designed for numerical values of the features. Thus, we
  wanted all the variables on the datasets to be numerical. When that was not
  possible, categorical variables were transformed to integers, as in these
  circumstances that seemed preferable to using One Hot Encoding.

  Table \ref{tab:dts-info} shows some information of the datasets used in this
  project. Most of them have few instances compared to some large-scale
  problems, but we expect the accuracy results will to be representative for
  bigger problems. Regarding the computation time, smaller datasets may not be
  the most appropriated to study the performance; for that it will be better to
  focus on MNIST and Fashion MNIST.

  Some preprocessing was done with the dataset Covertype, since the target classes
  were very unbalanced. As working with unbalanced classes is out of the scope
  of this project, we took a random subset of instances of each of the clases
  and discarded the rest.

  The features in all datasets have been normalised to have a mean of 0 and
  a variance of 1. This is a common practice to  normalise the contribution
  of the features of the problem, and also helps to reduce the time of
  some optimization problems.
\end{pre-delivery}
