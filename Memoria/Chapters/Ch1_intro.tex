% Chapter Template

\chapter{Introduction} % Main chapter title

\label{Chapter1} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

% \usepackage{mystyle}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------


\section{Problem to study}

\begin{pre-delivery}
  Supervised Learning uses statistical and mathematical models to predict a
  response variable from a classification or regression problem. Usually it does
  so by trying to minimise an error function.
  The better a model is, the higher the accuracy it will obtain on new, unseen
  data.

  Increasing the accuracy of the models is one of the main topics in the field.
  But it is not easy to achieve it. Usually, it comes at the cost of increasing the
  computation time to produce the model. Therefore, a trade-off needs to be
  made between the accuracy obtained and the training time.

  In this project we study some recent approaches to improve the trade-off of
  currently used Machine Learning methods. In particular, we study how
  Kernel approximation techniques could make some procedures feasible in
  very large datasets and also how to increase the accuracy of some models
  at the expense of some more time.
\end{pre-delivery}

% \section{Problem to solve}
%
% \begin{note}
%   \begin{itemize}
%     \item Trade-off between accuracy and train time is not good
%   \end{itemize}
% \end{note}
% \begin{pre-delivery}
%   Machine Learning has shown it can be very useful when trying to predict
%   a numerical or categorical variable based on some input data. It is able to
%   define mathematical and statistical models which can help us on a lot of
%   different fields, specially those that still need the presence of a human
%   to take some decision.
%
%   However, in most of the situations a trade-off needs to be made between the
%   ammount of precision in the predictions of the model (the accuracy) and the
%   ammount of time the model needs to define the prediction function based on
%   the data. There are many classification and regression problems that still
%   require very powerful computers and a lot of ``training time'' in order to
%   produce decent answers, and some of them still can't be solved with a
%   sufficient level of accuracy.
%
%   A lot of research is being done in the scientific community trying to improve this
%   trade-off. New models are defined, variations to old ones, techniques to
%   approximate more complex methods, etc. As the field progresses, it is
%   possible to deal with problems that where out the scope of Machile Learning.
%   But still, a lot of work needs to be done.
% \end{pre-delivery}



%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

% \section{Why is it important?}
%
% \begin{note}
%   \begin{itemize}
%     \item Avances en este campo permitirían usarlo en otras ciencias como medicina,
%     economía, sociedad
%     \item Muchas tareas que ahora tiene que hacer un humano podría hacerlas una
%     máquina, ahorrando tiempo y dinero
%   \end{itemize}
% \end{note}
%
% \begin{pre-delivery}
%   These days being able to learn from the data has many important applications.
%   Big companies make use of Machine Learning techniques in order to be more
%   efficient. Having good and cheap learning models helps them to have a
%   better, faster and more efficient decision making.
%
%
% \end{pre-delivery}


%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------

\section{Project proposal}

% \begin{note}
%   \begin{itemize}
%     \item Existe una batería de técnicas que son buenas, pero que nadie las
%     ha combinado. Son:
%     \begin{itemize}
%       \item Modelos simples
%       \item Ensembles
%       \item kernel trick
%       \item Aproximaciones de kernel
%     \end{itemize}
%     \item La propuesta es combinar todo esto para mejorar el trade-off
%     \item Sostenemos las siguientes hipótesis:
%     \begin{itemize}
%       \item Se podría hacer un ensemble con modelos distintos a DT
%       \item Se puede aproximar una RBF-SVM pero con el coste de una lineal
%       \item RFF + Bootstrap quizá es demasiado aleatorio
%       \item Los modelos que no se basan en productos escalares no se
%       beneficiarán tanto de usar RFF
%     \end{itemize}
%     \item Lo que se hará en cada capítulo del trabajo
%   \end{itemize}
% \end{note}

\begin{pre-delivery}
  The current development of Machine Learning has opened many fronts and
  techniques trying to solve many of the difficulties that the field has.

  One known issue is the Bias--Variance dilemma. While solving a classification
  or regression problem, the expected generalization error of a model is the
  sum of three error terms: the squared bias, the variance and the irreducible error.
  While the latter, as the name suggests, cannot be reduced, since it is
  caused by the inherent random noise in the data, the other two terms seem
  to have an inverse proportion: trying to reduce one of them increases the
  other most of the times.
  % In an attempt to reduce both of them (or at
  % least their sum) there have been developed some ensemble methods.
  Ensemble methods were developed in an attempt to reduce both of them (or at
  least their sum).
  Although they can outperform some models
  % Although they
  % tend to show good results,
  their usage is mostly restricted to a small subset of all available models.
  % their usage is mostly restricted to a small set of models.
  This is because they only increase significantly the accuracy on unstable
  methods.

  Another advance in Machine Learning has been the usage of kernel methods. They
  are useful to implicitly transform the data into another feature space with better properties,
  such as a clear dividing margin between classes of data. They are very
  effective, but their high computational costs has caused their use to be limited to just
  some specific problems or with a small number of instances. There are some
  less expensive approaches to approximate these methods, but they are not
  widely used.

  And then there is a collection of classical algorithms which have the
  advantage of being very simple and straightforward, although they don't
  usually get the highests scores.

  There is a collection of techniques that have shown some good results by their
  own, but still they haven't been tested in combination with the others. If we
  could mix some of these methods, maybe we could find new Machine Learning
  methods, with better accuracy or trade-off.

  In this project, we try some combinations of currently known techniques which
  could produce better results or show new useful model designs. On the one
  hand, we try to extend the usage of ensemble methods to new basic models.
  Currently, it doesn't make much sense to train an ensemble of Support Vector
  Machines or of Logistic Regression models, because they are so stable and
  most of the estimators would predict the same answer. We propose the use of
  random kernel approximations such as Random Fourier Features (RFF) or the
  \Nys\ method to increase the unstability of these models and thus be able to
  succesfully train and ansemble with them, hopefully increasing the accuracy
  of a single one.

  On the other hand, the usage of these Random Kernel approximations could
  allow us to use some methods which right now are not accessible. Support
  Vector Machines cannot be used with non-linear Kernels such as the Radial
  Basis Function (RBF) kernel on big datasets, since the cost is
  % very high.
  $\mathcal{O}(n^3)$
  with the number of instances.
  But if
  we transform the data to some space almost equivalent to the one of the
  RBF kernel, we can use a Linear Support Vector Machine, which is less
  expensive to train, and achieve a similar accuracy. In fact, with the usage
  of an ensemble, the results could be improved. Similar approaches have
  already been studied, and they have showed good results \cite{svm_rff}
  \cite{Zhang2017StackedKN} \cite{rahimi2008random}.

  For this study, we have formulated some hypothesis and we try to confirm or
  refuse them based on the experimental results. The hypothesis are:
  \begin{enumerate}
    % \item It is possible to achieve a similar accuracy to a SVM with the RBF
    % kernel but with less training time by using RFF or the \Nys\ method with
    % a linear SVM.
    \item An SVM using RFF or \Nys\ could achieve an accuracy similar to
    using the RBF kernel but with much less training time.
    % \item It is possible to increase the accuracy of a Logistic Regression
    % model or a SVM by training and ensemble of them with the usage of RFF
    % and the \Nys\ method.
    \item Training an ensemble of SVMs or Logistic Regression models using
    RFF or \Nys\ could increase the accuracy of a single estimator.
    % \item Curently used bagging ensemble method could show a bad behaviour
    % in combination with RFF of \Nys\ method due to an excess of Randomness,
    % caused by the Bootstrap together with the random mapping.
    \item Mixing a random mapping with the Bootstrap (from Bagging) could cause
    an excess of randomness and hence a bad accuracy compared to just using
    % one ofthem.
    the random mapping.
    \item Basic algorithms which are not based on the dot product of the
    input data such as the Decision Tree will not benefit so much of the
    usage of RFF and \Nys\ than those that do, like SVM or
    Logistic Regression.
  \end{enumerate}

  In the following pages we show an experimental set up to check these
  hypothesis and the results of the experiments. In Chapter \ref{Chapter2} we
  explain some of the Machine Learning concepts which are needed to understand
  the rest of the project. In \ref{Chapter3} we explain with more detail how
  this study is developed and what experiments are executed. In \ref{Chapter4}
  we show the results obtained with the experiments and discuss the hypothesis
  suggested based on them. In \ref{Chapter5} we present the conclussions of
  this project and propose some future work related to this topic. Finally,
  in Chapter \ref{Chapter6} we present a sustainability report of this project.

\end{pre-delivery}
