% Chapter Template

\chapter{Introduction} % Main chapter title

\label{Chapter1} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

% \usepackage{mystyle}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Problem to solve}

\begin{note}
  \begin{itemize}
    \item Trade-off between accuracy and train time is not good
  \end{itemize}
\end{note}
\begin{pre-delivery}
  Machine Learning has shown it can be very useful when trying to predict
  a numerical or categorical variable based on some input data. It is able to
  define mathematical and statistical models which can help us on a lot of
  different fields, specially those that still need the presence of a human
  to take some decision.

  However, in most of the situations a trade-off needs to be made between the
  ammount of precision in the predictions of the model (the accuracy) and the
  ammount of time the model needs to define the prediction function based on
  the data. There are many classification and regression problems that still
  require very powerful computers and a lot of ``training time'' in order to
  produce decent answers, and some of them still can't be solved with a
  sufficient level of accuracy.

  A lot of research is being done in the scientific community trying to improve this
  trade-off. New models are defined, variations to old ones, techniques to
  approximate more complex methods, etc. As the field progresses, it is
  possible to deal with problems that where out the scope of Machile Learning.
  But still, a lot of work needs to be done.
\end{pre-delivery}



%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Why is it important?}

\begin{note}
  \begin{itemize}
    \item Avances en este campo permitirían usarlo en otras ciencias como medicina,
    economía, sociedad
    \item Muchas tareas que ahora tiene que hacer un humano podría hacerlas una
    máquina, ahorrando tiempo y dinero
  \end{itemize}
\end{note}

\begin{pre-delivery}
  These days being able to learn from the data has many important applications.
  Big companies make use of Machine Learning techniques in order to be more
  efficient. Having good and cheap learning models helps them to have a
  better, faster and more efficient decision making.


\end{pre-delivery}


%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------

\section{Project proposal}

\begin{note}
  \begin{itemize}
    \item Existe una batería de técnicas que son buenas, pero que nadie las
    ha combinado. Son:
    \begin{itemize}
      \item Modelos simples
      \item Ensembles
      \item kernel trick
      \item Aproximaciones de kernel
    \end{itemize}
    \item La propuesta es combinar todo esto para mejorar el trade-off
    \item Sostenemos las siguientes hipótesis:
    \begin{itemize}
      \item Se podría hacer un ensemble con modelos distintos a DT
      \item Se puede aproximar una RBF-SVM pero con el coste de una lineal
      \item RFF + Bootstrap quizá es demasiado aleatorio
      \item Los modelos que no se basan en productos escalares no se
      beneficiarán tanto de usar RFF
    \end{itemize}
    \item Lo que se hará en cada capítulo del trabajo
  \end{itemize}
\end{note}

\begin{pre-delivery}
  The current development of Machine Learning has opened many fronts and
  techniques trying to solve many of the difficulties that the field has.

  One known issue is the Bias-Variance dilemma. While solving a classification
  or regression problem, the expected generalization error of the models is the
  sum of three error terms: the bias, the variance and the irreductible error.
  While the last one, as the name sugests, can not be reduced, since it is
  caused by the inherent random noise in the data, the other two terms seem
  to have an inverse proportion: trying to reduce one of them increases the
  other one most of the times. In an attempt to reduce both of them (or at
  least their sum) there have been developed some ensemble methods. While they
  tend to show good results, their usage is mostly restricted to a small set of
  models. This is because they tend to work better on unstable
  models.

  Another advance in Machine Learning has been the usage of kernel methods. They
  are useful to transform the data into another dimension with better properties,
  such as a clear dividing margin between classes of data. They are very
  effective, but their high computational costs has caused their use to be limited to just
  some specific problems or with a small number of instances. There are some
  less expensive approaches to approximate this methods, but they are not
  widely used.

  And then there is a collection of classical algorithms which tend to have the
  advantage of being very simple and straightforward, although they don't
  usually get the highests scores.

  There is a collection of techniques that have shown some good results by their
  own, but still they haven't been tested in combination with the others. If we
  could mix some of these methods, maybe we could find new Machine Learning
  methods, with better accuracy or trade-off.

  

\end{pre-delivery}
