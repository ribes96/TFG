@inproceedings{caputo2002appearance,
  title={Appearance-based object recognition using SVMs: which kernel should I use?},
  author={Caputo, Barbara and Sim, K and Furesjo, F and Smola, Alex},
  booktitle={Proc of NIPS workshop on Statistical methods for computational experiments in visual processing and computer vision, Whistler},
  volume={2002},
  year={2002}
}

@article{Zhang2017StackedKN,
  title={Stacked Kernel Network},
  author={Shuai Zhang and Jianxin Li and Pengtao Xie and Yingchun Zhang and Minglai Shao and Haoyi Zhou and Mengyi Yan},
  journal={CoRR},
  year={2017},
  volume={abs/1711.09219}
}

@incollection{NIPS2000_1866,
title = {Using the Nystr\"{o}m Method to Speed Up Kernel Machines},
author = {Christopher K. I. Williams and Matthias Seeger},
booktitle = {Advances in Neural Information Processing Systems 13},
editor = {T. K. Leen and T. G. Dietterich and V. Tresp},
pages = {682--688},
year = {2001},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/1866-using-the-nystrom-method-to-speed-up-kernel-machines.pdf}
}



@Article{Cortes1995,
author="Cortes, Corinna
and Vapnik, Vladimir",
title="Support-vector networks",
journal="Machine Learning",
year="1995",
month="Sep",
day="01",
volume="20",
number="3",
pages="273--297",
abstract="Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.",
issn="1573-0565",
doi="10.1007/BF00994018",
url="https://doi.org/10.1007/BF00994018"
}



@Article{Breiman2001,
author="Breiman, Leo",
title="Random Forests",
journal="Machine Learning",
year="2001",
month="Oct",
day="01",
volume="45",
number="1",
pages="5--32",
abstract="Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",
issn="1573-0565",
doi="10.1023/A:1010933404324",
url="https://doi.org/10.1023/A:1010933404324"
}


@InProceedings{pmlr-v28-le13,
  title = 	 {Fastfood - Computing Hilbert Space Expansions in loglinear time},
  author = 	 {Quoc Le and Tamas Sarlos and Alexander Smola},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {244--252},
  year = 	 {2013},
  editor = 	 {Sanjoy Dasgupta and David McAllester},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/le13.pdf},
  url = 	 {http://proceedings.mlr.press/v28/le13.html},
  abstract = 	 {Fast nonlinear function classes are crucial for nonparametric estimation, such as in kernel methods. This paper proposes an improvement to random kitchen sinks that offers significantly faster computation in log-linear time without sacrificing accuracy. Furthermore, we show how one may adjust the regularization properties of the kernel simply by changing the spectral distribution of the projection matrix. We provide experimental results which show that even for for moderately small problems we already achieve two orders of magnitude faster computation and three orders of magnitude lower memory footprint.  }
}


@article{jin2013improved,
  title={Improved bounds for the Nystr{\"o}m method with application to kernel classification},
  author={Jin, Rong and Yang, Tianbao and Mahdavi, Mehrdad and Li, Yu-Feng and Zhou, Zhi-Hua},
  journal={IEEE Transactions on Information Theory},
  volume={59},
  number={10},
  pages={6939--6949},
  year={2013},
  publisher={IEEE}
}


@inproceedings{yang2012nystrom,
  title={Nystr{\"o}m method vs random fourier features: A theoretical and empirical comparison},
  author={Yang, Tianbao and Li, Yu-Feng and Mahdavi, Mehrdad and Jin, Rong and Zhou, Zhi-Hua},
  booktitle={Advances in neural information processing systems},
  pages={476--484},
  year={2012}
}

@inproceedings{rahimi2008random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={1177--1184},
  year={2008}
}

@article{breiman1996bagging,
  title={Bagging predictors},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={24},
  number={2},
  pages={123--140},
  year={1996},
  publisher={Springer}
}

@article{schapire1990strength,
  title={The strength of weak learnability},
  author={Schapire, Robert E},
  journal={Machine learning},
  volume={5},
  number={2},
  pages={197--227},
  year={1990},
  publisher={Springer}
}

@book{nystrom1925numerische,
  title={{\"U}ber die numerische Integration von Differentialgleichungen:(Mitgeteilt am 23. Sept. 1925 von E. Lindel{\"o}f und KF Sundman)},
  author={Nystr{\"o}m, Evert Johannes},
  year={1925},
  publisher={Societas scientiarum Fennica}
}
